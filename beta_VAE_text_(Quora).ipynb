{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of beta_vae_text_final(Quora).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOPt2AupjTBr",
        "colab_type": "code",
        "outputId": "2448f00e-2ae8-4682-c5f4-408bb80f1424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "\n",
        "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout, CuDNNLSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from scipy import spatial\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiFaswSIPhD",
        "colab_type": "code",
        "outputId": "6d1267c0-23bf-4e11-b7fd-f1e1b40d9d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5b4ru2I0l6B",
        "colab_type": "code",
        "outputId": "fe29e99f-7219-47a0-845b-83e1d15634dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip 'drive/My Drive/train.csv.zip' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUqwygR7Frjl",
        "colab_type": "code",
        "outputId": "b4667606-b9d2-4511-b398-00e9ec9512c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BASE_DIR = 'drive/My Drive/'\n",
        "TRAIN_DATA_FILE = 'train.csv'\n",
        "GLOVE_EMBEDDING = BASE_DIR + 'Glove_Embeddings/glove.6B.300d.txt'\n",
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "MAX_NB_WORDS = 2000\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "texts = [] \n",
        "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        if len(values[3].split()) <= MAX_SEQUENCE_LENGTH:\n",
        "            texts.append(values[3])\n",
        "        if len(values[4].split()) <= MAX_SEQUENCE_LENGTH:\n",
        "            texts.append(values[4])\n",
        "print('Found %s texts in train.csv' % len(texts))\n",
        "n_sents = len(texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 783944 texts in train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-3lD5MvImut",
        "colab_type": "code",
        "outputId": "8aacc743-f958-4261-f505-a8a1c3bc803c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tokenizer = Tokenizer(MAX_NB_WORDS+1, oov_token='unk') #+1 for 'unk' token\n",
        "tokenizer.fit_on_texts(texts)\n",
        "print('Found %s unique tokens' % len(tokenizer.word_index))\n",
        "## **Key Step** to make it work correctly otherwise drops OOV tokens anyway!\n",
        "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS} # <= because tokenizer is 1 indexed\n",
        "tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1\n",
        "word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n",
        "index2word = {v: k for k, v in word_index.items()}\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data_1.shape)\n",
        "NB_WORDS = (min(tokenizer.num_words, len(word_index))+1) #+1 for zero padding \n",
        "\n",
        "data_val = data_1[775000:783000]\n",
        "data_train = data_1[:775000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 91443 unique tokens\n",
            "Shape of data tensor: (783944, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgU-IX5WP-Gk",
        "colab_type": "code",
        "outputId": "576eb193-2573-4b54-d512-0fb449da6607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(data_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    3    4    2\n",
            " 1236   59 1236 2001    8  572    9  774  371    9   36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlEs-sC6270h",
        "colab_type": "code",
        "outputId": "db756e50-3eaa-40b7-dad9-0a16a3dd6637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "for i in word_index:\n",
        "  if word_index[i] == 2003:\n",
        "    print(i)\n",
        "print(word_index.items())\n",
        "print(index2word)\n",
        "print(texts[0])\n",
        "print(sequences[0])\n",
        "print(data_1[0])\n",
        "print(tokenizer.num_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('unk', 2001), ('the', 2), ('what', 3), ('is', 4), ('how', 5), ('a', 6), ('i', 7), ('to', 8), ('in', 9), ('do', 10), ('of', 11), ('are', 12), ('and', 13), ('can', 14), ('for', 15), ('you', 16), ('why', 17), ('best', 18), ('my', 19), ('it', 20), ('on', 21), ('does', 22), ('which', 23), ('some', 24), ('or', 25), ('be', 26), ('if', 27), ('get', 28), ('should', 29), ('with', 30), ('have', 31), ('that', 32), ('an', 33), ('your', 34), ('from', 35), ('india', 36), ('will', 37), ('people', 38), ('who', 39), ('like', 40), ('when', 41), ('good', 42), ('at', 43), ('there', 44), ('would', 45), ('between', 46), ('about', 47), ('as', 48), ('most', 49), ('quora', 50), ('one', 51), ('way', 52), ('make', 53), ('did', 54), ('not', 55), ('where', 56), ('we', 57), ('life', 58), ('by', 59), ('any', 60), ('was', 61), ('money', 62), ('so', 63), ('time', 64), ('after', 65), ('difference', 66), ('learn', 67), ('know', 68), ('they', 69), (\"what's\", 70), ('me', 71), ('new', 72), ('this', 73), ('has', 74), ('use', 75), ('much', 76), ('think', 77), ('their', 78), ('many', 79), ('ever', 80), ('indian', 81), ('work', 82), ('someone', 83), ('find', 84), ('trump', 85), ('all', 86), ('than', 87), ('become', 88), ('more', 89), ('online', 90), ('without', 91), ('world', 92), ('better', 93), ('start', 94), ('first', 95), ('english', 96), ('other', 97), ('out', 98), ('am', 99), ('mean', 100), ('2016', 101), ('job', 102), ('into', 103), ('year', 104), ('us', 105), ('possible', 106), ('could', 107), ('love', 108), ('day', 109), ('2', 110), ('feel', 111), ('questions', 112), ('notes', 113), ('take', 114), ('up', 115), ('things', 116), ('500', 117), ('ways', 118), ('1', 119), ('want', 120), ('donald', 121), ('weight', 122), (\"don't\", 123), ('1000', 124), ('phone', 125), ('buy', 126), ('engineering', 127), ('were', 128), ('really', 129), ('used', 130), ('account', 131), ('improve', 132), ('go', 133), ('long', 134), ('google', 135), ('person', 136), ('books', 137), ('but', 138), ('number', 139), ('lose', 140), ('language', 141), ('using', 142), ('being', 143), ('movie', 144), ('facebook', 145), ('3', 146), ('sex', 147), ('business', 148), ('girl', 149), ('stop', 150), ('thing', 151), ('old', 152), ('book', 153), ('different', 154), ('black', 155), ('compare', 156), ('free', 157), ('war', 158), ('them', 159), ('question', 160), ('programming', 161), ('need', 162), ('movies', 163), ('president', 164), ('his', 165), ('instagram', 166), ('see', 167), ('he', 168), ('going', 169), ('happen', 170), ('examples', 171), ('prepare', 172), ('been', 173), ('real', 174), ('app', 175), ('its', 176), ('under', 177), ('women', 178), ('before', 179), ('change', 180), ('android', 181), ('help', 182), ('5', 183), ('learning', 184), ('important', 185), ('computer', 186), ('iphone', 187), ('still', 188), ('college', 189), ('rs', 190), ('win', 191), ('website', 192), ('c', 193), ('company', 194), ('over', 195), ('no', 196), ('live', 197), ('system', 198), ('country', 199), ('hillary', 200), ('ask', 201), ('data', 202), ('increase', 203), (\"i'm\", 204), ('clinton', 205), ('bad', 206), ('10', 207), ('had', 208), ('only', 209), ('top', 210), ('just', 211), ('study', 212), ('her', 213), ('while', 214), ('now', 215), ('made', 216), ('years', 217), ('read', 218), ('science', 219), ('earn', 220), ('s', 221), ('software', 222), ('through', 223), ('our', 224), ('water', 225), ('school', 226), ('during', 227), ('back', 228), ('mobile', 229), ('word', 230), ('government', 231), ('math', 232), ('exam', 233), ('energy', 234), ('com', 235), ('card', 236), ('high', 237), ('4', 238), ('men', 239), ('car', 240), ('two', 241), ('name', 242), ('university', 243), ('companies', 244), ('average', 245), ('anyone', 246), ('same', 247), ('2017', 248), ('say', 249), ('true', 250), ('laptop', 251), ('give', 252), ('china', 253), ('student', 254), (\"can't\", 255), ('web', 256), ('getting', 257), ('earth', 258), ('meaning', 259), ('hair', 260), ('home', 261), ('social', 262), ('safe', 263), ('interview', 264), ('youtube', 265), ('watch', 266), ('video', 267), ('travel', 268), ('answer', 269), ('career', 270), ('look', 271), ('doing', 272), ('right', 273), ('rupee', 274), ('come', 275), ('write', 276), ('pakistan', 277), ('tell', 278), ('students', 279), ('bank', 280), ('rid', 281), ('usa', 282), ('eat', 283), ('cost', 284), ('happens', 285), ('favorite', 286), ('food', 287), ('non', 288), ('password', 289), ('girls', 290), ('own', 291), ('b', 292), ('game', 293), ('place', 294), ('tv', 295), ('employees', 296), ('service', 297), ('done', 298), ('major', 299), ('human', 300), ('days', 301), ('u', 302), ('mind', 303), ('man', 304), ('countries', 305), ('off', 306), ('x', 307), ('big', 308), ('download', 309), ('play', 310), ('she', 311), ('election', 312), ('relationship', 313), ('whatsapp', 314), ('affect', 315), ('effects', 316), ('tips', 317), ('history', 318), ('chinese', 319), ('differences', 320), ('experience', 321), ('having', 322), ('friends', 323), ('exist', 324), ('process', 325), ('skills', 326), ('support', 327), ('believe', 328), ('white', 329), ('interesting', 330), ('great', 331), ('java', 332), ('places', 333), ('marketing', 334), ('guy', 335), ('engineer', 336), ('future', 337), ('even', 338), ('friend', 339), ('tech', 340), ('mechanical', 341), ('state', 342), ('working', 343), ('delhi', 344), ('6', 345), ('test', 346), ('against', 347), ('internet', 348), ('makes', 349), ('music', 350), ('body', 351), ('age', 352), ('end', 353), ('then', 354), ('7', 355), ('american', 356), (\"doesn't\", 357), ('police', 358), ('myself', 359), ('create', 360), ('visit', 361), ('states', 362), ('united', 363), ('email', 364), ('making', 365), ('god', 366), ('hard', 367), ('windows', 368), ('song', 369), ('worth', 370), ('market', 371), ('every', 372), ('class', 373), ('culture', 374), ('very', 375), ('power', 376), ('next', 377), ('control', 378), ('gmail', 379), ('last', 380), ('e', 381), ('answers', 382), ('series', 383), ('hotel', 384), ('fat', 385), ('too', 386), ('actually', 387), ('economy', 388), ('salary', 389), ('available', 390), ('modi', 391), ('america', 392), ('considered', 393), ('these', 394), ('common', 395), ('code', 396), ('month', 397), (\"you've\", 398), ('development', 399), ('universities', 400), ('self', 401), ('die', 402), ('writing', 403), ('delete', 404), ('keep', 405), ('girlfriend', 406), ('review', 407), ('course', 408), ('hack', 409), ('worst', 410), ('looking', 411), ('pay', 412), ('site', 413), ('jobs', 414), ('happened', 415), ('woman', 416), ('purpose', 417), ('songs', 418), ('never', 419), ('him', 420), ('services', 421), ('presidential', 422), ('differ', 423), ('universe', 424), ('deal', 425), ('majors', 426), ('behind', 427), ('light', 428), ('near', 429), ('each', 430), ('reduce', 431), ('cat', 432), ('games', 433), ('type', 434), ('note', 435), ('bangalore', 436), ('popular', 437), ('around', 438), ('design', 439), ('open', 440), ('something', 441), ('dark', 442), ('education', 443), ('build', 444), ('digital', 445), ('apply', 446), ('websites', 447), ('kind', 448), ('8', 449), ('per', 450), ('another', 451), ('list', 452), ('score', 453), ('always', 454), ('mba', 455), ('show', 456), ('facts', 457), ('private', 458), ('biggest', 459), ('post', 460), ('sentence', 461), ('space', 462), ('traffic', 463), ('civil', 464), ('cause', 465), ('living', 466), ('technology', 467), ('height', 468), ('ca', 469), ('causes', 470), ('seen', 471), ('parents', 472), ('2000', 473), ('speed', 474), ('hate', 475), ('law', 476), ('management', 477), ('center', 478), ('easily', 479), ('apps', 480), ('current', 481), ('idea', 482), ('currency', 483), ('today', 484), ('problem', 485), ('period', 486), ('international', 487), ('reason', 488), ('public', 489), ('main', 490), ('air', 491), ('sleep', 492), ('run', 493), ('got', 494), ('apple', 495), ('benefits', 496), ('instead', 497), ('indians', 498), ('sites', 499), ('compared', 500), ('city', 501), ('asked', 502), ('join', 503), ('pregnant', 504), ('python', 505), ('drug', 506), ('battle', 507), ('recover', 508), ('death', 509), ('program', 510), ('gain', 511), ('wrong', 512), ('date', 513), ('machine', 514), ('media', 515), ('wear', 516), ('search', 517), ('call', 518), ('such', 519), ('order', 520), ('0', 521), ('canada', 522), ('based', 523), ('degree', 524), ('successful', 525), ('ban', 526), ('videos', 527), ('well', 528), ('products', 529), ('fast', 530), ('california', 531), ('remove', 532), ('decision', 533), ('add', 534), ('family', 535), ('stay', 536), ('stock', 537), ('overcome', 538), ('choose', 539), ('health', 540), ('iit', 541), ('preparation', 542), ('months', 543), ('story', 544), ('views', 545), ('humans', 546), ('foreign', 547), ('down', 548), ('startup', 549), ('ms', 550), ('fall', 551), ('dog', 552), ('banning', 553), ('easiest', 554), ('part', 555), ('face', 556), ('small', 557), ('terms', 558), ('medical', 559), ('solar', 560), ('vote', 561), ('alcohol', 562), ('legal', 563), ('plan', 564), ('ones', 565), ('value', 566), ('coaching', 567), ('jee', 568), ('theory', 569), ('created', 570), ('porn', 571), ('invest', 572), ('house', 573), ('child', 574), ('less', 575), ('times', 576), ('known', 577), ('speak', 578), ('grow', 579), ('rate', 580), ('yourself', 581), ('effect', 582), ('amazon', 583), ('boyfriend', 584), ('profile', 585), ('gate', 586), ('form', 587), ('level', 588), ('sydney', 589), ('000', 590), ('move', 591), ('found', 592), ('ideas', 593), ('bollywood', 594), ('research', 595), ('project', 596), ('pros', 597), ('called', 598), ('snapchat', 599), ('cons', 600), ('product', 601), ('drive', 602), ('famous', 603), ('ias', 604), ('physics', 605), ('able', 606), ('uk', 607), ('quality', 608), ('normal', 609), ('point', 610), ('must', 611), ('improvement', 612), ('advantages', 613), ('visa', 614), ('australia', 615), ('desert', 616), ('easy', 617), ('100', 618), ('view', 619), ('faster', 620), ('20', 621), ('given', 622), ('correct', 623), ('russia', 624), ('side', 625), ('both', 626), ('credit', 627), ('provider', 628), ('suicide', 629), ('star', 630), ('matter', 631), ('group', 632), ('rupees', 633), ('2015', 634), ('sell', 635), ('germany', 636), ('advice', 637), ('application', 638), ('courses', 639), ('south', 640), ('cell', 641), ('green', 642), ('others', 643), ('safety', 644), ('institute', 645), ('information', 646), ('follow', 647), ('solve', 648), ('languages', 649), ('send', 650), ('suitable', 651), (\"it's\", 652), ('guys', 653), ('security', 654), ('required', 655), ('effective', 656), ('pro', 657), ('mumbai', 658), ('daily', 659), ('full', 660), ('kill', 661), ('marks', 662), ('single', 663), ('size', 664), (\"someone's\", 665), ('m', 666), ('similar', 667), ('avoid', 668), ('function', 669), ('9', 670), ('short', 671), ('healthy', 672), ('source', 673), ('deleted', 674), ('distance', 675), ('impact', 676), ('again', 677), ('options', 678), ('messages', 679), ('etc', 680), ('blood', 681), ('investment', 682), ('uber', 683), ('night', 684), ('marriage', 685), ('children', 686), ('general', 687), ('low', 688), ('iq', 689), ('happy', 690), ('studying', 691), ('industry', 692), ('tax', 693), ('jio', 694), ('blog', 695), ('beautiful', 696), ('exactly', 697), ('file', 698), ('week', 699), ('types', 700), ('oil', 701), ('chemical', 702), ('developer', 703), ('anything', 704), ('sim', 705), ('field', 706), ('hours', 707), ('problems', 708), ('put', 709), ('often', 710), ('force', 711), ('because', 712), ('cold', 713), ('dogs', 714), ('photos', 715), ('mass', 716), ('develop', 717), ('term', 718), ('electrical', 719), ('colleges', 720), ('japanese', 721), ('pc', 722), ('across', 723), ('proposed', 724), ('model', 725), ('talk', 726), ('brain', 727), ('also', 728), ('12', 729), ('reasons', 730), (\"didn't\", 731), ('words', 732), ('dream', 733), ('turn', 734), ('engine', 735), ('hindi', 736), ('gay', 737), ('loss', 738), ('net', 739), ('japan', 740), ('macbook', 741), ('precautions', 742), ('15', 743), ('eating', 744), ('area', 745), ('picture', 746), ('care', 747), ('county', 748), ('handling', 749), ('fix', 750), ('north', 751), ('training', 752), ('lost', 753), ('check', 754), ('prime', 755), ('price', 756), ('left', 757), ('party', 758), ('religion', 759), ('ios', 760), ('moment', 761), ('page', 762), ('americans', 763), ('topics', 764), ('hyderabad', 765), ('calculate', 766), ('fight', 767), ('numbers', 768), ('narendra', 769), ('heard', 770), ('pass', 771), ('installation', 772), ('mix', 773), ('share', 774), ('those', 775), ('three', 776), ('panel', 777), ('basic', 778), ('political', 779), ('recruit', 780), ('set', 781), ('quickly', 782), ('message', 783), ('rehab', 784), ('30', 785), ('gold', 786), ('likes', 787), ('samsung', 788), ('moon', 789), ('penis', 790), ('demonetization', 791), ('balance', 792), ('inr', 793), ('network', 794), ('british', 795), ('shotguns', 796), ('blowing', 797), ('exams', 798), ('female', 799), ('graduate', 800), ('convert', 801), ('nra', 802), ('belly', 803), ('started', 804), ('store', 805), ('related', 806), (\"isn't\", 807), ('red', 808), ('line', 809), ('national', 810), ('coming', 811), ('lot', 812), ('understand', 813), ('male', 814), ('hollywood', 815), ('film', 816), ('pune', 817), ('communication', 818), ('letter', 819), ('office', 820), ('earthquake', 821), ('text', 822), ('won', 823), ('depression', 824), ('access', 825), ('rich', 826), ('smart', 827), ('team', 828), ('drink', 829), ('speaking', 830), ('animals', 831), ('meet', 832), ('contact', 833), ('personal', 834), ('late', 835), ('twitter', 836), ('boy', 837), ('second', 838), ('changed', 839), ('resources', 840), ('everyone', 841), ('grads', 842), ('transfer', 843), ('corruption', 844), ('bill', 845), ('obama', 846), ('army', 847), ('chances', 848), ('news', 849), ('likely', 850), ('determine', 851), ('address', 852), ('inpatient', 853), ('galaxy', 854), ('train', 855), ('explain', 856), ('scope', 857), ('ex', 858), ('color', 859), ('following', 860), ('users', 861), ('paid', 862), ('role', 863), ('muslims', 864), ('married', 865), ('disadvantages', 866), ('policy', 867), ('reading', 868), ('structure', 869), ('season', 870), ('enough', 871), ('vs', 872), ('opinion', 873), ('significance', 874), ('greatest', 875), ('hire', 876), ('financial', 877), ('singapore', 878), ('everything', 879), ('linux', 880), ('diet', 881), ('needing', 882), ('block', 883), ('german', 884), ('leave', 885), ('blocked', 886), ('paper', 887), ('solution', 888), ('photo', 889), ('gift', 890), ('islam', 891), ('past', 892), ('europe', 893), ('offer', 894), ('taken', 895), ('temperatures', 896), ('couples', 897), ('moral', 898), ('taking', 899), ('difficult', 900), ('option', 901), ('tools', 902), ('meth', 903), ('officer', 904), ('seo', 905), ('laws', 906), ('capital', 907), ('few', 908), ('infinite', 909), ('screen', 910), ('useful', 911), ('blue', 912), ('n', 913), ('once', 914), ('professional', 915), ('yes', 916), ('break', 917), ('natural', 918), ('pictures', 919), ('method', 920), ('limit', 921), ('skin', 922), ('within', 923), ('muslim', 924), ('smartphone', 925), ('favourite', 926), ('visiting', 927), ('negative', 928), ('fake', 929), ('chance', 930), ('nuclear', 931), ('away', 932), ('allowed', 933), (\"you're\", 934), ('studies', 935), ('simple', 936), ('sun', 937), ('property', 938), ('kerala', 939), ('strategy', 940), ('wife', 941), ('track', 942), ('personality', 943), ('charge', 944), ('estate', 945), ('please', 946), ('names', 947), ('spend', 948), ('internship', 949), ('crush', 950), ('formula', 951), ('character', 952), ('attack', 953), ('pain', 954), ('knowledge', 955), ('tea', 956), ('dating', 957), ('football', 958), ('50', 959), ('board', 960), ('microsoft', 961), ('admission', 962), ('thinking', 963), ('buying', 964), ('wifi', 965), ('followers', 966), ('cards', 967), ('french', 968), ('inside', 969), ('smoking', 970), ('western', 971), ('cut', 972), ('income', 973), ('minister', 974), ('11', 975), ('elections', 976), ('install', 977), ('military', 978), ('fear', 979), ('16', 980), ('stories', 981), ('accounts', 982), ('written', 983), ('commit', 984), ('evidence', 985), ('corporate', 986), ('says', 987), ('unmarried', 988), ('mac', 989), ('systems', 990), ('special', 991), ('pokémon', 992), ('memory', 993), ('staff', 994), ('rank', 995), ('sound', 996), ('crack', 997), ('watching', 998), ('anxiety', 999), ('starting', 1000), ('percentage', 1001), ('illegal', 1002), ('t', 1003), ('modern', 1004), ('final', 1005), ('hand', 1006), ('least', 1007), ('necessary', 1008), ('bring', 1009), ('camera', 1010), ('upsc', 1011), ('preparing', 1012), ('prefer', 1013), ('viewed', 1014), ('already', 1015), ('intelligence', 1016), ('early', 1017), ('positions', 1018), ('phd', 1019), ('beginner', 1020), ('birthday', 1021), ('amount', 1022), ('deep', 1023), ('battery', 1024), (\"trump's\", 1025), ('anime', 1026), ('neet', 1027), ('eye', 1028), ('chemistry', 1029), ('phones', 1030), ('middle', 1031), ('applications', 1032), ('try', 1033), ('save', 1034), ('device', 1035), ('reset', 1036), ('let', 1037), ('brand', 1038), ('olympics', 1039), ('coding', 1040), ('land', 1041), ('user', 1042), ('ssc', 1043), ('works', 1044), ('schools', 1045), ('exercise', 1046), ('society', 1047), ('shows', 1048), ('marked', 1049), ('describe', 1050), ('dangerous', 1051), ('d', 1052), ('towards', 1053), ('running', 1054), ('fastest', 1055), ('yahoo', 1056), ('wars', 1057), ('18', 1058), ('front', 1059), ('presidency', 1060), ('eyes', 1061), ('masturbation', 1062), ('technical', 1063), ('close', 1064), ('expect', 1065), ('positive', 1066), ('feeling', 1067), ('details', 1068), ('vacuum', 1069), ('flat', 1070), ('prevent', 1071), ('version', 1072), ('cancer', 1073), ('station', 1074), ('gaming', 1075), ('funniest', 1076), ('harry', 1077), ('playing', 1078), ('sbi', 1079), ('york', 1080), ('candy', 1081), ('files', 1082), ('dead', 1083), ('said', 1084), ('provide', 1085), ('pressure', 1086), ('latest', 1087), ('art', 1088), ('cure', 1089), ('hp', 1090), ('bang', 1091), ('marry', 1092), ('chennai', 1093), ('core', 1094), ('interested', 1095), ('passport', 1096), ('among', 1097), ('harassment', 1098), ('currently', 1099), ('classes', 1100), ('gre', 1101), ('imported', 1102), ('since', 1103), ('commerce', 1104), ('economics', 1105), ('secret', 1106), ('events', 1107), ('treat', 1108), ('little', 1109), ('determined', 1110), ('resolution', 1111), ('morning', 1112), ('hacks', 1113), ('kids', 1114), ('attractive', 1115), ('mathematics', 1116), ('projects', 1117), ('ancient', 1118), ('case', 1119), ('sea', 1120), ('doctor', 1121), ('practice', 1122), ('acne', 1123), ('gravity', 1124), ('consider', 1125), ('cars', 1126), ('global', 1127), ('cricket', 1128), ('update', 1129), ('teach', 1130), ('spotify', 1131), ('minimum', 1132), ('growth', 1133), ('13', 1134), ('clean', 1135), ('coffee', 1136), ('gst', 1137), ('weed', 1138), ('engineers', 1139), ('far', 1140), ('forgot', 1141), ('politics', 1142), ('id', 1143), ('image', 1144), ('prospects', 1145), ('highest', 1146), ('below', 1147), ('taffy', 1148), ('insurance', 1149), ('according', 1150), ('super', 1151), ('san', 1152), ('expected', 1153), ('actor', 1154), ('cheap', 1155), ('becoming', 1156), ('million', 1157), ('poor', 1158), ('yet', 1159), ('young', 1160), ('head', 1161), ('else', 1162), ('cs', 1163), ('gas', 1164), ('disorder', 1165), ('advanced', 1166), ('example', 1167), ('here', 1168), ('trading', 1169), ('hole', 1170), ('tata', 1171), ('mail', 1172), ('recovery', 1173), ('tamil', 1174), ('g', 1175), ('husband', 1176), ('beginners', 1177), ('goa', 1178), ('mother', 1179), ('spanish', 1180), ('importance', 1181), ('opportunities', 1182), ('manager', 1183), ('mark', 1184), ('channel', 1185), ('proof', 1186), ('hacker', 1187), ('interest', 1188), ('alone', 1189), ('departments', 1190), ('drinking', 1191), ('zero', 1192), ('suggest', 1193), ('uses', 1194), ('third', 1195), ('milk', 1196), ('prove', 1197), ('present', 1198), ('range', 1199), ('may', 1200), ('gadgets', 1201), ('electronics', 1202), ('contrast', 1203), ('league', 1204), ('potter', 1205), ('potential', 1206), ('baby', 1207), ('thrones', 1208), ('clear', 1209), ('startups', 1210), ('remember', 1211), ('higher', 1212), ('voice', 1213), ('root', 1214), ('14', 1215), ('powerful', 1216), ('becomes', 1217), ('heart', 1218), ('korea', 1219), ('competitive', 1220), ('israel', 1221), ('plus', 1222), ('asian', 1223), ('pounds', 1224), ('court', 1225), ('license', 1226), ('content', 1227), ('building', 1228), ('animal', 1229), ('acid', 1230), ('though', 1231), ('survive', 1232), ('cse', 1233), ('2014', 1234), ('wants', 1235), ('step', 1236), ('summer', 1237), ('saltwater', 1238), ('steps', 1239), ('abroad', 1240), ('lesser', 1241), ('pursue', 1242), ('act', 1243), ('together', 1244), ('losing', 1245), ('r', 1246), ('large', 1247), ('features', 1248), ('writer', 1249), ('dreams', 1250), ('hacking', 1251), ('status', 1252), ('sign', 1253), ('4g', 1254), ('due', 1255), ('cash', 1256), ('resolutions', 1257), ('unusual', 1258), ('pm', 1259), ('complete', 1260), ('functions', 1261), ('kashmir', 1262), ('enjoy', 1263), ('wish', 1264), ('procedure', 1265), ('cheating', 1266), ('intelligent', 1267), ('okay', 1268), ('cbse', 1269), ('l', 1270), ('gets', 1271), ('economic', 1272), ('psychology', 1273), ('budget', 1274), (\"won't\", 1275), ('native', 1276), ('wall', 1277), ('quit', 1278), ('tourist', 1279), ('manage', 1280), ('finance', 1281), ('rather', 1282), ('driving', 1283), ('campus', 1284), ('embarrassing', 1285), ('issues', 1286), ('answered', 1287), ('might', 1288), ('bike', 1289), ('developed', 1290), ('boys', 1291), ('fire', 1292), ('mit', 1293), ('changing', 1294), ('partner', 1295), ('completing', 1296), ('tool', 1297), ('player', 1298), ('needed', 1299), ('isis', 1300), ('testing', 1301), ('temperature', 1302), ('strongest', 1303), ('mexico', 1304), ('existence', 1305), ('surgical', 1306), ('17', 1307), ('addiction', 1308), ('sales', 1309), ('pan', 1310), ('success', 1311), ('born', 1312), ('whether', 1313), ('speech', 1314), ('banned', 1315), ('hot', 1316), ('forget', 1317), ('os', 1318), ('korean', 1319), ('creative', 1320), ('comes', 1321), (\"india's\", 1322), ('allow', 1323), ('everyday', 1324), ('cities', 1325), ('cells', 1326), ('masters', 1327), ('connect', 1328), ('physical', 1329), ('shoes', 1330), ('12th', 1331), ('scientific', 1332), ('lead', 1333), ('hurt', 1334), ('object', 1335), ('east', 1336), ('outside', 1337), ('sahara', 1338), ('trip', 1339), ('invented', 1340), ('relationships', 1341), ('teeth', 1342), ('banks', 1343), ('needs', 1344), ('debate', 1345), ('hindu', 1346), (\"aren't\", 1347), ('programmer', 1348), ('php', 1349), ('restaurants', 1350), ('javascript', 1351), ('elected', 1352), ('25', 1353), ('multiple', 1354), ('father', 1355), ('teacher', 1356), ('planet', 1357), ('cgl', 1358), ('analysis', 1359), ('server', 1360), ('expensive', 1361), (\"they're\", 1362), ('birth', 1363), ('anger', 1364), ('stand', 1365), ('crime', 1366), ('adult', 1367), ('regret', 1368), ('specific', 1369), ('ball', 1370), ('stocks', 1371), ('reality', 1372), ('london', 1373), ('weeks', 1374), ('messenger', 1375), ('whom', 1376), ('planning', 1377), ('cool', 1378), ('platform', 1379), ('key', 1380), ('kvpy', 1381), ('nothing', 1382), (\"i've\", 1383), ('touch', 1384), ('fresher', 1385), ('longer', 1386), ('sexual', 1387), ('bible', 1388), ('log', 1389), ('themselves', 1390), ('approach', 1391), ('drivers', 1392), ('heat', 1393), ('strong', 1394), ('convince', 1395), ('factors', 1396), ('double', 1397), ('room', 1398), ('location', 1399), ('phrase', 1400), ('sports', 1401), ('whole', 1402), ('cats', 1403), ('guitar', 1404), ('population', 1405), ('reservation', 1406), ('stars', 1407), ('designer', 1408), ('caste', 1409), ('mbbs', 1410), ('cheapest', 1411), ('recommend', 1412), ('master', 1413), ('y', 1414), ('accept', 1415), ('programs', 1416), ('episode', 1417), ('tcs', 1418), ('completely', 1419), ('maximum', 1420), ('emotions', 1421), ('climate', 1422), ('gifts', 1423), ('permanent', 1424), ('trying', 1425), ('k', 1426), ('issue', 1427), ('experiences', 1428), ('replace', 1429), ('database', 1430), ('romantic', 1431), ('businesses', 1432), ('chat', 1433), ('harvard', 1434), ('rest', 1435), ('flight', 1436), ('means', 1437), ('customer', 1438), ('taste', 1439), ('devices', 1440), ('ice', 1441), ('amazing', 1442), ('philippines', 1443), ('turkey', 1444), ('choice', 1445), ('sent', 1446), ('dollars', 1447), ('characteristics', 1448), ('cambodia', 1449), ('commercial', 1450), ('square', 1451), ('weirdest', 1452), ('anti', 1453), ('hitler', 1454), ('stream', 1455), ('perfect', 1456), ('topic', 1457), ('algorithms', 1458), ('curb', 1459), ('ticket', 1460), ('protein', 1461), ('via', 1462), ('saying', 1463), ('equation', 1464), ('po', 1465), ('listen', 1466), ('release', 1467), ('investing', 1468), ('standard', 1469), ('graduation', 1470), ('gym', 1471), ('torrent', 1472), ('planets', 1473), ('smell', 1474), ('workout', 1475), ('position', 1476), ('historical', 1477), ('f', 1478), ('stress', 1479), ('dc', 1480), ('rights', 1481), ('european', 1482), ('winning', 1483), ('24', 1484), ('profit', 1485), ('definition', 1486), ('khan', 1487), ('hill', 1488), ('3g', 1489), ('affected', 1490), ('jesus', 1491), ('waves', 1492), ('electric', 1493), ('react', 1494), ('shopping', 1495), ('treatment', 1496), ('ram', 1497), ('confidence', 1498), ('puppy', 1499), ('fund', 1500), ('begin', 1501), ('direct', 1502), ('scientist', 1503), ('straight', 1504), ('aliens', 1505), ('foods', 1506), ('accepted', 1507), ('certificate', 1508), ('hour', 1509), ('thoughts', 1510), ('gps', 1511), ('analytics', 1512), ('lord', 1513), ('maths', 1514), ('appear', 1515), ('material', 1516), ('cook', 1517), ('muscle', 1518), ('sleeping', 1519), ('meant', 1520), ('table', 1521), ('total', 1522), ('plant', 1523), ('philosophy', 1524), ('lower', 1525), ('subject', 1526), ('fun', 1527), ('five', 1528), ('event', 1529), ('signs', 1530), ('chicken', 1531), ('recent', 1532), ('aspects', 1533), ('link', 1534), ('restaurant', 1535), ('racist', 1536), ('iii', 1537), ('urine', 1538), ('active', 1539), ('40', 1540), ('giving', 1541), ('parts', 1542), ('candidate', 1543), ('benefit', 1544), ('burn', 1545), ('asking', 1546), ('mains', 1547), ('ok', 1548), ('background', 1549), ('performance', 1550), ('quickbooks', 1551), ('drop', 1552), ('cultural', 1553), ('loan', 1554), ('medicine', 1555), ('joke', 1556), ('ipad', 1557), ('package', 1558), ('rbi', 1559), ('moving', 1560), ('original', 1561), ('conversation', 1562), ('branch', 1563), ('pokemon', 1564), ('p', 1565), ('soon', 1566), ('reach', 1567), ('trust', 1568), ('upload', 1569), ('reliable', 1570), ('requirements', 1571), ('films', 1572), ('symptoms', 1573), ('slow', 1574), ('hotels', 1575), ('netflix', 1576), ('fly', 1577), ('clothes', 1578), ('receive', 1579), ('coast', 1580), ('cyrus', 1581), ('grade', 1582), ('navy', 1583), ('race', 1584), ('quantum', 1585), ('france', 1586), ('v', 1587), ('driver', 1588), ('production', 1589), ('dubai', 1590), ('certain', 1591), ('investors', 1592), ('attracted', 1593), ('domain', 1594), ('mistry', 1595), ('naturally', 1596), ('talking', 1597), ('lines', 1598), ('gmat', 1599), ('king', 1600), ('mental', 1601), ('undergraduate', 1602), ('reaction', 1603), ('jews', 1604), ('four', 1605), ('nice', 1606), ('cloud', 1607), ('actors', 1608), ('typical', 1609), ('origin', 1610), ('reliance', 1611), ('banking', 1612), ('promote', 1613), ('wins', 1614), ('blogs', 1615), ('strike', 1616), ('met', 1617), ('unique', 1618), ('marijuana', 1619), ('21', 1620), ('achieve', 1621), ('equal', 1622), ('resume', 1623), ('afraid', 1624), ('attractions', 1625), ('meat', 1626), ('west', 1627), ('decide', 1628), (\"clinton's\", 1629), ('francisco', 1630), ('essay', 1631), ('somme', 1632), ('serve', 1633), ('came', 1634), ('cutoff', 1635), ('older', 1636), ('learned', 1637), ('son', 1638), ('fluently', 1639), ('effectively', 1640), ('street', 1641), ('horror', 1642), ('return', 1643), ('goes', 1644), ('structures', 1645), ('round', 1646), ('seem', 1647), ('methods', 1648), ('surgery', 1649), ('trade', 1650), ('operating', 1651), ('peace', 1652), ('literature', 1653), ('fact', 1654), ('local', 1655), ('novels', 1656), ('funds', 1657), ('voltage', 1658), ('wordpress', 1659), ('sometimes', 1660), ('comments', 1661), ('knowing', 1662), ('overrated', 1663), ('scam', 1664), ('received', 1665), ('funding', 1666), ('tall', 1667), ('sector', 1668), ('pregnancy', 1669), ('russian', 1670), ('generally', 1671), ('cream', 1672), ('html', 1673), ('motor', 1674), ('placement', 1675), ('africa', 1676), ('debit', 1677), ('rule', 1678), ('moto', 1679), ('pdf', 1680), ('demand', 1681), ('caused', 1682), ('union', 1683), ('characters', 1684), ('told', 1685), ('hide', 1686), ('hacked', 1687), ('grades', 1688), ('sense', 1689), (\"modi's\", 1690), ('fiction', 1691), ('measure', 1692), ('regular', 1693), ('selling', 1694), ('novel', 1695), ('concept', 1696), ('died', 1697), ('spoken', 1698), ('visitors', 1699), ('symbol', 1700), ('rock', 1701), ('feelings', 1702), ('switch', 1703), ('architecture', 1704), ('notice', 1705), ('co', 1706), ('hiring', 1707), ('wave', 1708), ('lives', 1709), ('colors', 1710), ('ii', 1711), ('weakest', 1712), ('results', 1713), ('cycle', 1714), ('3d', 1715), ('organic', 1716), ('minutes', 1717), ('feed', 1718), ('pre', 1719), ('led', 1720), ('empire', 1721), ('images', 1722), ('hit', 1723), ('produce', 1724), ('define', 1725), ('phase', 1726), ('6s', 1727), ('released', 1728), ('focus', 1729), ('formed', 1730), ('wake', 1731), ('treated', 1732), ('paying', 1733), ('15000', 1734), ('truth', 1735), ('api', 1736), ('species', 1737), ('half', 1738), ('brown', 1739), ('skill', 1740), ('stupid', 1741), ('chrome', 1742), ('cover', 1743), ('sentences', 1744), ('gandhi', 1745), ('steel', 1746), ('usually', 1747), ('calls', 1748), ('purchase', 1749), ('eggs', 1750), ('calories', 1751), ('kolkata', 1752), ('identify', 1753), ('mistake', 1754), ('metal', 1755), ('extra', 1756), ('goal', 1757), ('loves', 1758), ('similarities', 1759), ('technologies', 1760), ('batman', 1761), ('hear', 1762), ('upgrade', 1763), ('rates', 1764), ('sa', 1765), ('gun', 1766), ('excel', 1767), ('analyst', 1768), ('lyrics', 1769), ('citizen', 1770), ('glass', 1771), ('presence', 1772), ('ibps', 1773), ('wedding', 1774), ('base', 1775), ('bit', 1776), ('removed', 1777), ('mars', 1778), ('usb', 1779), ('campaign', 1780), ('payment', 1781), ('sugar', 1782), ('reviews', 1783), ('built', 1784), ('ac', 1785), ('taller', 1786), ('fit', 1787), ('entire', 1788), ('various', 1789), ('experienced', 1790), ('handle', 1791), ('electricity', 1792), ('style', 1793), ('exchange', 1794), ('fail', 1795), ('error', 1796), ('killed', 1797), ('permanently', 1798), ('materials', 1799), ('pilot', 1800), ('algorithm', 1801), ('virtual', 1802), ('easier', 1803), ('christmas', 1804), ('naruto', 1805), ('failure', 1806), ('cannot', 1807), ('actress', 1808), ('xbox', 1809), ('gravitational', 1810), ('solutions', 1811), ('unknown', 1812), ('depressed', 1813), ('scratch', 1814), ('accurate', 1815), ('democracy', 1816), ('linkedin', 1817), ('certification', 1818), ('probability', 1819), ('bar', 1820), ('wait', 1821), ('speaker', 1822), ('became', 1823), ('iim', 1824), ('introvert', 1825), ('shot', 1826), ('airport', 1827), ('anybody', 1828), ('fi', 1829), ('dhoni', 1830), ('launch', 1831), ('truly', 1832), ('supply', 1833), ('valley', 1834), ('groups', 1835), ('plane', 1836), ('tinder', 1837), ('worse', 1838), ('dry', 1839), ('community', 1840), ('changes', 1841), ('emails', 1842), ('laptops', 1843), ('hold', 1844), ('lie', 1845), ('artificial', 1846), ('played', 1847), ('result', 1848), ('alive', 1849), ('regarding', 1850), ('anymore', 1851), ('prices', 1852), ('box', 1853), ('harmful', 1854), ('alternative', 1855), (\"shouldn't\", 1856), ('bed', 1857), ('applying', 1858), ('theories', 1859), ('biology', 1860), ('temple', 1861), ('sad', 1862), ('scale', 1863), ('networking', 1864), ('exercises', 1865), ('along', 1866), ('mutual', 1867), ('gender', 1868), ('six', 1869), ('js', 1870), ('placements', 1871), ('smoke', 1872), ('evil', 1873), ('raise', 1874), ('scientifically', 1875), ('suddenly', 1876), ('tricks', 1877), ('hardest', 1878), ('plants', 1879), ('redmi', 1880), ('colour', 1881), ('atheists', 1882), ('carbon', 1883), ('activities', 1884), ('especially', 1885), ('shall', 1886), ('forces', 1887), ('register', 1888), ('developing', 1889), ('report', 1890), ('nose', 1891), ('offline', 1892), ('connection', 1893), ('fish', 1894), ('rules', 1895), ('areas', 1896), ('constitution', 1897), ('rent', 1898), ('road', 1899), ('print', 1900), ('intel', 1901), ('occur', 1902), ('largest', 1903), ('ece', 1904), ('entrance', 1905), ('kejriwal', 1906), ('piece', 1907), ('count', 1908), ('equivalent', 1909), ('zone', 1910), ('ip', 1911), ('200', 1912), ('legally', 1913), ('respond', 1914), ('named', 1915), ('pick', 1916), ('beat', 1917), (\"one's\", 1918), ('piano', 1919), ('professor', 1920), ('evolution', 1921), ('situation', 1922), ('revenue', 1923), ('efficient', 1924), ('suggestions', 1925), ('stage', 1926), ('freedom', 1927), ('catch', 1928), ('employee', 1929), ('christian', 1930), ('almost', 1931), ('consequences', 1932), ('visitor', 1933), ('basis', 1934), ('scrapping', 1935), ('pride', 1936), ('un', 1937), ('members', 1938), ('players', 1939), ('raw', 1940), ('invited', 1941), ('atheist', 1942), ('prison', 1943), ('dna', 1944), ('length', 1945), ('designing', 1946), ('hands', 1947), ('democratic', 1948), ('central', 1949), ('asia', 1950), ('besides', 1951), ('spouse', 1952), ('weather', 1953), ('century', 1954), ('strikes', 1955), ('gobi', 1956), ('mistakes', 1957), ('o', 1958), ('salt', 1959), ('developers', 1960), ('statement', 1961), ('feet', 1962), ('action', 1963), ('individuals', 1964), ('pronunciation', 1965), ('affordable', 1966), ('lewis', 1967), ('fashion', 1968), ('dress', 1969), ('15k', 1970), ('involved', 1971), ('itself', 1972), ('daughter', 1973), ('iron', 1974), ('environment', 1975), ('tickets', 1976), ('joining', 1977), ('limited', 1978), ('framework', 1979), ('router', 1980), ('ad', 1981), ('ads', 1982), ('frac', 1983), ('liquid', 1984), ('virus', 1985), ('canadian', 1986), ('stopped', 1987), ('loose', 1988), ('entrepreneur', 1989), ('unlock', 1990), ('venture', 1991), ('density', 1992), ('masturbate', 1993), ('entry', 1994), ('brands', 1995), ('scene', 1996), ('ideal', 1997), ('industrial', 1998), ('faux', 1999), ('pas', 2000)])\n",
            "{2001: 'unk', 2: 'the', 3: 'what', 4: 'is', 5: 'how', 6: 'a', 7: 'i', 8: 'to', 9: 'in', 10: 'do', 11: 'of', 12: 'are', 13: 'and', 14: 'can', 15: 'for', 16: 'you', 17: 'why', 18: 'best', 19: 'my', 20: 'it', 21: 'on', 22: 'does', 23: 'which', 24: 'some', 25: 'or', 26: 'be', 27: 'if', 28: 'get', 29: 'should', 30: 'with', 31: 'have', 32: 'that', 33: 'an', 34: 'your', 35: 'from', 36: 'india', 37: 'will', 38: 'people', 39: 'who', 40: 'like', 41: 'when', 42: 'good', 43: 'at', 44: 'there', 45: 'would', 46: 'between', 47: 'about', 48: 'as', 49: 'most', 50: 'quora', 51: 'one', 52: 'way', 53: 'make', 54: 'did', 55: 'not', 56: 'where', 57: 'we', 58: 'life', 59: 'by', 60: 'any', 61: 'was', 62: 'money', 63: 'so', 64: 'time', 65: 'after', 66: 'difference', 67: 'learn', 68: 'know', 69: 'they', 70: \"what's\", 71: 'me', 72: 'new', 73: 'this', 74: 'has', 75: 'use', 76: 'much', 77: 'think', 78: 'their', 79: 'many', 80: 'ever', 81: 'indian', 82: 'work', 83: 'someone', 84: 'find', 85: 'trump', 86: 'all', 87: 'than', 88: 'become', 89: 'more', 90: 'online', 91: 'without', 92: 'world', 93: 'better', 94: 'start', 95: 'first', 96: 'english', 97: 'other', 98: 'out', 99: 'am', 100: 'mean', 101: '2016', 102: 'job', 103: 'into', 104: 'year', 105: 'us', 106: 'possible', 107: 'could', 108: 'love', 109: 'day', 110: '2', 111: 'feel', 112: 'questions', 113: 'notes', 114: 'take', 115: 'up', 116: 'things', 117: '500', 118: 'ways', 119: '1', 120: 'want', 121: 'donald', 122: 'weight', 123: \"don't\", 124: '1000', 125: 'phone', 126: 'buy', 127: 'engineering', 128: 'were', 129: 'really', 130: 'used', 131: 'account', 132: 'improve', 133: 'go', 134: 'long', 135: 'google', 136: 'person', 137: 'books', 138: 'but', 139: 'number', 140: 'lose', 141: 'language', 142: 'using', 143: 'being', 144: 'movie', 145: 'facebook', 146: '3', 147: 'sex', 148: 'business', 149: 'girl', 150: 'stop', 151: 'thing', 152: 'old', 153: 'book', 154: 'different', 155: 'black', 156: 'compare', 157: 'free', 158: 'war', 159: 'them', 160: 'question', 161: 'programming', 162: 'need', 163: 'movies', 164: 'president', 165: 'his', 166: 'instagram', 167: 'see', 168: 'he', 169: 'going', 170: 'happen', 171: 'examples', 172: 'prepare', 173: 'been', 174: 'real', 175: 'app', 176: 'its', 177: 'under', 178: 'women', 179: 'before', 180: 'change', 181: 'android', 182: 'help', 183: '5', 184: 'learning', 185: 'important', 186: 'computer', 187: 'iphone', 188: 'still', 189: 'college', 190: 'rs', 191: 'win', 192: 'website', 193: 'c', 194: 'company', 195: 'over', 196: 'no', 197: 'live', 198: 'system', 199: 'country', 200: 'hillary', 201: 'ask', 202: 'data', 203: 'increase', 204: \"i'm\", 205: 'clinton', 206: 'bad', 207: '10', 208: 'had', 209: 'only', 210: 'top', 211: 'just', 212: 'study', 213: 'her', 214: 'while', 215: 'now', 216: 'made', 217: 'years', 218: 'read', 219: 'science', 220: 'earn', 221: 's', 222: 'software', 223: 'through', 224: 'our', 225: 'water', 226: 'school', 227: 'during', 228: 'back', 229: 'mobile', 230: 'word', 231: 'government', 232: 'math', 233: 'exam', 234: 'energy', 235: 'com', 236: 'card', 237: 'high', 238: '4', 239: 'men', 240: 'car', 241: 'two', 242: 'name', 243: 'university', 244: 'companies', 245: 'average', 246: 'anyone', 247: 'same', 248: '2017', 249: 'say', 250: 'true', 251: 'laptop', 252: 'give', 253: 'china', 254: 'student', 255: \"can't\", 256: 'web', 257: 'getting', 258: 'earth', 259: 'meaning', 260: 'hair', 261: 'home', 262: 'social', 263: 'safe', 264: 'interview', 265: 'youtube', 266: 'watch', 267: 'video', 268: 'travel', 269: 'answer', 270: 'career', 271: 'look', 272: 'doing', 273: 'right', 274: 'rupee', 275: 'come', 276: 'write', 277: 'pakistan', 278: 'tell', 279: 'students', 280: 'bank', 281: 'rid', 282: 'usa', 283: 'eat', 284: 'cost', 285: 'happens', 286: 'favorite', 287: 'food', 288: 'non', 289: 'password', 290: 'girls', 291: 'own', 292: 'b', 293: 'game', 294: 'place', 295: 'tv', 296: 'employees', 297: 'service', 298: 'done', 299: 'major', 300: 'human', 301: 'days', 302: 'u', 303: 'mind', 304: 'man', 305: 'countries', 306: 'off', 307: 'x', 308: 'big', 309: 'download', 310: 'play', 311: 'she', 312: 'election', 313: 'relationship', 314: 'whatsapp', 315: 'affect', 316: 'effects', 317: 'tips', 318: 'history', 319: 'chinese', 320: 'differences', 321: 'experience', 322: 'having', 323: 'friends', 324: 'exist', 325: 'process', 326: 'skills', 327: 'support', 328: 'believe', 329: 'white', 330: 'interesting', 331: 'great', 332: 'java', 333: 'places', 334: 'marketing', 335: 'guy', 336: 'engineer', 337: 'future', 338: 'even', 339: 'friend', 340: 'tech', 341: 'mechanical', 342: 'state', 343: 'working', 344: 'delhi', 345: '6', 346: 'test', 347: 'against', 348: 'internet', 349: 'makes', 350: 'music', 351: 'body', 352: 'age', 353: 'end', 354: 'then', 355: '7', 356: 'american', 357: \"doesn't\", 358: 'police', 359: 'myself', 360: 'create', 361: 'visit', 362: 'states', 363: 'united', 364: 'email', 365: 'making', 366: 'god', 367: 'hard', 368: 'windows', 369: 'song', 370: 'worth', 371: 'market', 372: 'every', 373: 'class', 374: 'culture', 375: 'very', 376: 'power', 377: 'next', 378: 'control', 379: 'gmail', 380: 'last', 381: 'e', 382: 'answers', 383: 'series', 384: 'hotel', 385: 'fat', 386: 'too', 387: 'actually', 388: 'economy', 389: 'salary', 390: 'available', 391: 'modi', 392: 'america', 393: 'considered', 394: 'these', 395: 'common', 396: 'code', 397: 'month', 398: \"you've\", 399: 'development', 400: 'universities', 401: 'self', 402: 'die', 403: 'writing', 404: 'delete', 405: 'keep', 406: 'girlfriend', 407: 'review', 408: 'course', 409: 'hack', 410: 'worst', 411: 'looking', 412: 'pay', 413: 'site', 414: 'jobs', 415: 'happened', 416: 'woman', 417: 'purpose', 418: 'songs', 419: 'never', 420: 'him', 421: 'services', 422: 'presidential', 423: 'differ', 424: 'universe', 425: 'deal', 426: 'majors', 427: 'behind', 428: 'light', 429: 'near', 430: 'each', 431: 'reduce', 432: 'cat', 433: 'games', 434: 'type', 435: 'note', 436: 'bangalore', 437: 'popular', 438: 'around', 439: 'design', 440: 'open', 441: 'something', 442: 'dark', 443: 'education', 444: 'build', 445: 'digital', 446: 'apply', 447: 'websites', 448: 'kind', 449: '8', 450: 'per', 451: 'another', 452: 'list', 453: 'score', 454: 'always', 455: 'mba', 456: 'show', 457: 'facts', 458: 'private', 459: 'biggest', 460: 'post', 461: 'sentence', 462: 'space', 463: 'traffic', 464: 'civil', 465: 'cause', 466: 'living', 467: 'technology', 468: 'height', 469: 'ca', 470: 'causes', 471: 'seen', 472: 'parents', 473: '2000', 474: 'speed', 475: 'hate', 476: 'law', 477: 'management', 478: 'center', 479: 'easily', 480: 'apps', 481: 'current', 482: 'idea', 483: 'currency', 484: 'today', 485: 'problem', 486: 'period', 487: 'international', 488: 'reason', 489: 'public', 490: 'main', 491: 'air', 492: 'sleep', 493: 'run', 494: 'got', 495: 'apple', 496: 'benefits', 497: 'instead', 498: 'indians', 499: 'sites', 500: 'compared', 501: 'city', 502: 'asked', 503: 'join', 504: 'pregnant', 505: 'python', 506: 'drug', 507: 'battle', 508: 'recover', 509: 'death', 510: 'program', 511: 'gain', 512: 'wrong', 513: 'date', 514: 'machine', 515: 'media', 516: 'wear', 517: 'search', 518: 'call', 519: 'such', 520: 'order', 521: '0', 522: 'canada', 523: 'based', 524: 'degree', 525: 'successful', 526: 'ban', 527: 'videos', 528: 'well', 529: 'products', 530: 'fast', 531: 'california', 532: 'remove', 533: 'decision', 534: 'add', 535: 'family', 536: 'stay', 537: 'stock', 538: 'overcome', 539: 'choose', 540: 'health', 541: 'iit', 542: 'preparation', 543: 'months', 544: 'story', 545: 'views', 546: 'humans', 547: 'foreign', 548: 'down', 549: 'startup', 550: 'ms', 551: 'fall', 552: 'dog', 553: 'banning', 554: 'easiest', 555: 'part', 556: 'face', 557: 'small', 558: 'terms', 559: 'medical', 560: 'solar', 561: 'vote', 562: 'alcohol', 563: 'legal', 564: 'plan', 565: 'ones', 566: 'value', 567: 'coaching', 568: 'jee', 569: 'theory', 570: 'created', 571: 'porn', 572: 'invest', 573: 'house', 574: 'child', 575: 'less', 576: 'times', 577: 'known', 578: 'speak', 579: 'grow', 580: 'rate', 581: 'yourself', 582: 'effect', 583: 'amazon', 584: 'boyfriend', 585: 'profile', 586: 'gate', 587: 'form', 588: 'level', 589: 'sydney', 590: '000', 591: 'move', 592: 'found', 593: 'ideas', 594: 'bollywood', 595: 'research', 596: 'project', 597: 'pros', 598: 'called', 599: 'snapchat', 600: 'cons', 601: 'product', 602: 'drive', 603: 'famous', 604: 'ias', 605: 'physics', 606: 'able', 607: 'uk', 608: 'quality', 609: 'normal', 610: 'point', 611: 'must', 612: 'improvement', 613: 'advantages', 614: 'visa', 615: 'australia', 616: 'desert', 617: 'easy', 618: '100', 619: 'view', 620: 'faster', 621: '20', 622: 'given', 623: 'correct', 624: 'russia', 625: 'side', 626: 'both', 627: 'credit', 628: 'provider', 629: 'suicide', 630: 'star', 631: 'matter', 632: 'group', 633: 'rupees', 634: '2015', 635: 'sell', 636: 'germany', 637: 'advice', 638: 'application', 639: 'courses', 640: 'south', 641: 'cell', 642: 'green', 643: 'others', 644: 'safety', 645: 'institute', 646: 'information', 647: 'follow', 648: 'solve', 649: 'languages', 650: 'send', 651: 'suitable', 652: \"it's\", 653: 'guys', 654: 'security', 655: 'required', 656: 'effective', 657: 'pro', 658: 'mumbai', 659: 'daily', 660: 'full', 661: 'kill', 662: 'marks', 663: 'single', 664: 'size', 665: \"someone's\", 666: 'm', 667: 'similar', 668: 'avoid', 669: 'function', 670: '9', 671: 'short', 672: 'healthy', 673: 'source', 674: 'deleted', 675: 'distance', 676: 'impact', 677: 'again', 678: 'options', 679: 'messages', 680: 'etc', 681: 'blood', 682: 'investment', 683: 'uber', 684: 'night', 685: 'marriage', 686: 'children', 687: 'general', 688: 'low', 689: 'iq', 690: 'happy', 691: 'studying', 692: 'industry', 693: 'tax', 694: 'jio', 695: 'blog', 696: 'beautiful', 697: 'exactly', 698: 'file', 699: 'week', 700: 'types', 701: 'oil', 702: 'chemical', 703: 'developer', 704: 'anything', 705: 'sim', 706: 'field', 707: 'hours', 708: 'problems', 709: 'put', 710: 'often', 711: 'force', 712: 'because', 713: 'cold', 714: 'dogs', 715: 'photos', 716: 'mass', 717: 'develop', 718: 'term', 719: 'electrical', 720: 'colleges', 721: 'japanese', 722: 'pc', 723: 'across', 724: 'proposed', 725: 'model', 726: 'talk', 727: 'brain', 728: 'also', 729: '12', 730: 'reasons', 731: \"didn't\", 732: 'words', 733: 'dream', 734: 'turn', 735: 'engine', 736: 'hindi', 737: 'gay', 738: 'loss', 739: 'net', 740: 'japan', 741: 'macbook', 742: 'precautions', 743: '15', 744: 'eating', 745: 'area', 746: 'picture', 747: 'care', 748: 'county', 749: 'handling', 750: 'fix', 751: 'north', 752: 'training', 753: 'lost', 754: 'check', 755: 'prime', 756: 'price', 757: 'left', 758: 'party', 759: 'religion', 760: 'ios', 761: 'moment', 762: 'page', 763: 'americans', 764: 'topics', 765: 'hyderabad', 766: 'calculate', 767: 'fight', 768: 'numbers', 769: 'narendra', 770: 'heard', 771: 'pass', 772: 'installation', 773: 'mix', 774: 'share', 775: 'those', 776: 'three', 777: 'panel', 778: 'basic', 779: 'political', 780: 'recruit', 781: 'set', 782: 'quickly', 783: 'message', 784: 'rehab', 785: '30', 786: 'gold', 787: 'likes', 788: 'samsung', 789: 'moon', 790: 'penis', 791: 'demonetization', 792: 'balance', 793: 'inr', 794: 'network', 795: 'british', 796: 'shotguns', 797: 'blowing', 798: 'exams', 799: 'female', 800: 'graduate', 801: 'convert', 802: 'nra', 803: 'belly', 804: 'started', 805: 'store', 806: 'related', 807: \"isn't\", 808: 'red', 809: 'line', 810: 'national', 811: 'coming', 812: 'lot', 813: 'understand', 814: 'male', 815: 'hollywood', 816: 'film', 817: 'pune', 818: 'communication', 819: 'letter', 820: 'office', 821: 'earthquake', 822: 'text', 823: 'won', 824: 'depression', 825: 'access', 826: 'rich', 827: 'smart', 828: 'team', 829: 'drink', 830: 'speaking', 831: 'animals', 832: 'meet', 833: 'contact', 834: 'personal', 835: 'late', 836: 'twitter', 837: 'boy', 838: 'second', 839: 'changed', 840: 'resources', 841: 'everyone', 842: 'grads', 843: 'transfer', 844: 'corruption', 845: 'bill', 846: 'obama', 847: 'army', 848: 'chances', 849: 'news', 850: 'likely', 851: 'determine', 852: 'address', 853: 'inpatient', 854: 'galaxy', 855: 'train', 856: 'explain', 857: 'scope', 858: 'ex', 859: 'color', 860: 'following', 861: 'users', 862: 'paid', 863: 'role', 864: 'muslims', 865: 'married', 866: 'disadvantages', 867: 'policy', 868: 'reading', 869: 'structure', 870: 'season', 871: 'enough', 872: 'vs', 873: 'opinion', 874: 'significance', 875: 'greatest', 876: 'hire', 877: 'financial', 878: 'singapore', 879: 'everything', 880: 'linux', 881: 'diet', 882: 'needing', 883: 'block', 884: 'german', 885: 'leave', 886: 'blocked', 887: 'paper', 888: 'solution', 889: 'photo', 890: 'gift', 891: 'islam', 892: 'past', 893: 'europe', 894: 'offer', 895: 'taken', 896: 'temperatures', 897: 'couples', 898: 'moral', 899: 'taking', 900: 'difficult', 901: 'option', 902: 'tools', 903: 'meth', 904: 'officer', 905: 'seo', 906: 'laws', 907: 'capital', 908: 'few', 909: 'infinite', 910: 'screen', 911: 'useful', 912: 'blue', 913: 'n', 914: 'once', 915: 'professional', 916: 'yes', 917: 'break', 918: 'natural', 919: 'pictures', 920: 'method', 921: 'limit', 922: 'skin', 923: 'within', 924: 'muslim', 925: 'smartphone', 926: 'favourite', 927: 'visiting', 928: 'negative', 929: 'fake', 930: 'chance', 931: 'nuclear', 932: 'away', 933: 'allowed', 934: \"you're\", 935: 'studies', 936: 'simple', 937: 'sun', 938: 'property', 939: 'kerala', 940: 'strategy', 941: 'wife', 942: 'track', 943: 'personality', 944: 'charge', 945: 'estate', 946: 'please', 947: 'names', 948: 'spend', 949: 'internship', 950: 'crush', 951: 'formula', 952: 'character', 953: 'attack', 954: 'pain', 955: 'knowledge', 956: 'tea', 957: 'dating', 958: 'football', 959: '50', 960: 'board', 961: 'microsoft', 962: 'admission', 963: 'thinking', 964: 'buying', 965: 'wifi', 966: 'followers', 967: 'cards', 968: 'french', 969: 'inside', 970: 'smoking', 971: 'western', 972: 'cut', 973: 'income', 974: 'minister', 975: '11', 976: 'elections', 977: 'install', 978: 'military', 979: 'fear', 980: '16', 981: 'stories', 982: 'accounts', 983: 'written', 984: 'commit', 985: 'evidence', 986: 'corporate', 987: 'says', 988: 'unmarried', 989: 'mac', 990: 'systems', 991: 'special', 992: 'pokémon', 993: 'memory', 994: 'staff', 995: 'rank', 996: 'sound', 997: 'crack', 998: 'watching', 999: 'anxiety', 1000: 'starting', 1001: 'percentage', 1002: 'illegal', 1003: 't', 1004: 'modern', 1005: 'final', 1006: 'hand', 1007: 'least', 1008: 'necessary', 1009: 'bring', 1010: 'camera', 1011: 'upsc', 1012: 'preparing', 1013: 'prefer', 1014: 'viewed', 1015: 'already', 1016: 'intelligence', 1017: 'early', 1018: 'positions', 1019: 'phd', 1020: 'beginner', 1021: 'birthday', 1022: 'amount', 1023: 'deep', 1024: 'battery', 1025: \"trump's\", 1026: 'anime', 1027: 'neet', 1028: 'eye', 1029: 'chemistry', 1030: 'phones', 1031: 'middle', 1032: 'applications', 1033: 'try', 1034: 'save', 1035: 'device', 1036: 'reset', 1037: 'let', 1038: 'brand', 1039: 'olympics', 1040: 'coding', 1041: 'land', 1042: 'user', 1043: 'ssc', 1044: 'works', 1045: 'schools', 1046: 'exercise', 1047: 'society', 1048: 'shows', 1049: 'marked', 1050: 'describe', 1051: 'dangerous', 1052: 'd', 1053: 'towards', 1054: 'running', 1055: 'fastest', 1056: 'yahoo', 1057: 'wars', 1058: '18', 1059: 'front', 1060: 'presidency', 1061: 'eyes', 1062: 'masturbation', 1063: 'technical', 1064: 'close', 1065: 'expect', 1066: 'positive', 1067: 'feeling', 1068: 'details', 1069: 'vacuum', 1070: 'flat', 1071: 'prevent', 1072: 'version', 1073: 'cancer', 1074: 'station', 1075: 'gaming', 1076: 'funniest', 1077: 'harry', 1078: 'playing', 1079: 'sbi', 1080: 'york', 1081: 'candy', 1082: 'files', 1083: 'dead', 1084: 'said', 1085: 'provide', 1086: 'pressure', 1087: 'latest', 1088: 'art', 1089: 'cure', 1090: 'hp', 1091: 'bang', 1092: 'marry', 1093: 'chennai', 1094: 'core', 1095: 'interested', 1096: 'passport', 1097: 'among', 1098: 'harassment', 1099: 'currently', 1100: 'classes', 1101: 'gre', 1102: 'imported', 1103: 'since', 1104: 'commerce', 1105: 'economics', 1106: 'secret', 1107: 'events', 1108: 'treat', 1109: 'little', 1110: 'determined', 1111: 'resolution', 1112: 'morning', 1113: 'hacks', 1114: 'kids', 1115: 'attractive', 1116: 'mathematics', 1117: 'projects', 1118: 'ancient', 1119: 'case', 1120: 'sea', 1121: 'doctor', 1122: 'practice', 1123: 'acne', 1124: 'gravity', 1125: 'consider', 1126: 'cars', 1127: 'global', 1128: 'cricket', 1129: 'update', 1130: 'teach', 1131: 'spotify', 1132: 'minimum', 1133: 'growth', 1134: '13', 1135: 'clean', 1136: 'coffee', 1137: 'gst', 1138: 'weed', 1139: 'engineers', 1140: 'far', 1141: 'forgot', 1142: 'politics', 1143: 'id', 1144: 'image', 1145: 'prospects', 1146: 'highest', 1147: 'below', 1148: 'taffy', 1149: 'insurance', 1150: 'according', 1151: 'super', 1152: 'san', 1153: 'expected', 1154: 'actor', 1155: 'cheap', 1156: 'becoming', 1157: 'million', 1158: 'poor', 1159: 'yet', 1160: 'young', 1161: 'head', 1162: 'else', 1163: 'cs', 1164: 'gas', 1165: 'disorder', 1166: 'advanced', 1167: 'example', 1168: 'here', 1169: 'trading', 1170: 'hole', 1171: 'tata', 1172: 'mail', 1173: 'recovery', 1174: 'tamil', 1175: 'g', 1176: 'husband', 1177: 'beginners', 1178: 'goa', 1179: 'mother', 1180: 'spanish', 1181: 'importance', 1182: 'opportunities', 1183: 'manager', 1184: 'mark', 1185: 'channel', 1186: 'proof', 1187: 'hacker', 1188: 'interest', 1189: 'alone', 1190: 'departments', 1191: 'drinking', 1192: 'zero', 1193: 'suggest', 1194: 'uses', 1195: 'third', 1196: 'milk', 1197: 'prove', 1198: 'present', 1199: 'range', 1200: 'may', 1201: 'gadgets', 1202: 'electronics', 1203: 'contrast', 1204: 'league', 1205: 'potter', 1206: 'potential', 1207: 'baby', 1208: 'thrones', 1209: 'clear', 1210: 'startups', 1211: 'remember', 1212: 'higher', 1213: 'voice', 1214: 'root', 1215: '14', 1216: 'powerful', 1217: 'becomes', 1218: 'heart', 1219: 'korea', 1220: 'competitive', 1221: 'israel', 1222: 'plus', 1223: 'asian', 1224: 'pounds', 1225: 'court', 1226: 'license', 1227: 'content', 1228: 'building', 1229: 'animal', 1230: 'acid', 1231: 'though', 1232: 'survive', 1233: 'cse', 1234: '2014', 1235: 'wants', 1236: 'step', 1237: 'summer', 1238: 'saltwater', 1239: 'steps', 1240: 'abroad', 1241: 'lesser', 1242: 'pursue', 1243: 'act', 1244: 'together', 1245: 'losing', 1246: 'r', 1247: 'large', 1248: 'features', 1249: 'writer', 1250: 'dreams', 1251: 'hacking', 1252: 'status', 1253: 'sign', 1254: '4g', 1255: 'due', 1256: 'cash', 1257: 'resolutions', 1258: 'unusual', 1259: 'pm', 1260: 'complete', 1261: 'functions', 1262: 'kashmir', 1263: 'enjoy', 1264: 'wish', 1265: 'procedure', 1266: 'cheating', 1267: 'intelligent', 1268: 'okay', 1269: 'cbse', 1270: 'l', 1271: 'gets', 1272: 'economic', 1273: 'psychology', 1274: 'budget', 1275: \"won't\", 1276: 'native', 1277: 'wall', 1278: 'quit', 1279: 'tourist', 1280: 'manage', 1281: 'finance', 1282: 'rather', 1283: 'driving', 1284: 'campus', 1285: 'embarrassing', 1286: 'issues', 1287: 'answered', 1288: 'might', 1289: 'bike', 1290: 'developed', 1291: 'boys', 1292: 'fire', 1293: 'mit', 1294: 'changing', 1295: 'partner', 1296: 'completing', 1297: 'tool', 1298: 'player', 1299: 'needed', 1300: 'isis', 1301: 'testing', 1302: 'temperature', 1303: 'strongest', 1304: 'mexico', 1305: 'existence', 1306: 'surgical', 1307: '17', 1308: 'addiction', 1309: 'sales', 1310: 'pan', 1311: 'success', 1312: 'born', 1313: 'whether', 1314: 'speech', 1315: 'banned', 1316: 'hot', 1317: 'forget', 1318: 'os', 1319: 'korean', 1320: 'creative', 1321: 'comes', 1322: \"india's\", 1323: 'allow', 1324: 'everyday', 1325: 'cities', 1326: 'cells', 1327: 'masters', 1328: 'connect', 1329: 'physical', 1330: 'shoes', 1331: '12th', 1332: 'scientific', 1333: 'lead', 1334: 'hurt', 1335: 'object', 1336: 'east', 1337: 'outside', 1338: 'sahara', 1339: 'trip', 1340: 'invented', 1341: 'relationships', 1342: 'teeth', 1343: 'banks', 1344: 'needs', 1345: 'debate', 1346: 'hindu', 1347: \"aren't\", 1348: 'programmer', 1349: 'php', 1350: 'restaurants', 1351: 'javascript', 1352: 'elected', 1353: '25', 1354: 'multiple', 1355: 'father', 1356: 'teacher', 1357: 'planet', 1358: 'cgl', 1359: 'analysis', 1360: 'server', 1361: 'expensive', 1362: \"they're\", 1363: 'birth', 1364: 'anger', 1365: 'stand', 1366: 'crime', 1367: 'adult', 1368: 'regret', 1369: 'specific', 1370: 'ball', 1371: 'stocks', 1372: 'reality', 1373: 'london', 1374: 'weeks', 1375: 'messenger', 1376: 'whom', 1377: 'planning', 1378: 'cool', 1379: 'platform', 1380: 'key', 1381: 'kvpy', 1382: 'nothing', 1383: \"i've\", 1384: 'touch', 1385: 'fresher', 1386: 'longer', 1387: 'sexual', 1388: 'bible', 1389: 'log', 1390: 'themselves', 1391: 'approach', 1392: 'drivers', 1393: 'heat', 1394: 'strong', 1395: 'convince', 1396: 'factors', 1397: 'double', 1398: 'room', 1399: 'location', 1400: 'phrase', 1401: 'sports', 1402: 'whole', 1403: 'cats', 1404: 'guitar', 1405: 'population', 1406: 'reservation', 1407: 'stars', 1408: 'designer', 1409: 'caste', 1410: 'mbbs', 1411: 'cheapest', 1412: 'recommend', 1413: 'master', 1414: 'y', 1415: 'accept', 1416: 'programs', 1417: 'episode', 1418: 'tcs', 1419: 'completely', 1420: 'maximum', 1421: 'emotions', 1422: 'climate', 1423: 'gifts', 1424: 'permanent', 1425: 'trying', 1426: 'k', 1427: 'issue', 1428: 'experiences', 1429: 'replace', 1430: 'database', 1431: 'romantic', 1432: 'businesses', 1433: 'chat', 1434: 'harvard', 1435: 'rest', 1436: 'flight', 1437: 'means', 1438: 'customer', 1439: 'taste', 1440: 'devices', 1441: 'ice', 1442: 'amazing', 1443: 'philippines', 1444: 'turkey', 1445: 'choice', 1446: 'sent', 1447: 'dollars', 1448: 'characteristics', 1449: 'cambodia', 1450: 'commercial', 1451: 'square', 1452: 'weirdest', 1453: 'anti', 1454: 'hitler', 1455: 'stream', 1456: 'perfect', 1457: 'topic', 1458: 'algorithms', 1459: 'curb', 1460: 'ticket', 1461: 'protein', 1462: 'via', 1463: 'saying', 1464: 'equation', 1465: 'po', 1466: 'listen', 1467: 'release', 1468: 'investing', 1469: 'standard', 1470: 'graduation', 1471: 'gym', 1472: 'torrent', 1473: 'planets', 1474: 'smell', 1475: 'workout', 1476: 'position', 1477: 'historical', 1478: 'f', 1479: 'stress', 1480: 'dc', 1481: 'rights', 1482: 'european', 1483: 'winning', 1484: '24', 1485: 'profit', 1486: 'definition', 1487: 'khan', 1488: 'hill', 1489: '3g', 1490: 'affected', 1491: 'jesus', 1492: 'waves', 1493: 'electric', 1494: 'react', 1495: 'shopping', 1496: 'treatment', 1497: 'ram', 1498: 'confidence', 1499: 'puppy', 1500: 'fund', 1501: 'begin', 1502: 'direct', 1503: 'scientist', 1504: 'straight', 1505: 'aliens', 1506: 'foods', 1507: 'accepted', 1508: 'certificate', 1509: 'hour', 1510: 'thoughts', 1511: 'gps', 1512: 'analytics', 1513: 'lord', 1514: 'maths', 1515: 'appear', 1516: 'material', 1517: 'cook', 1518: 'muscle', 1519: 'sleeping', 1520: 'meant', 1521: 'table', 1522: 'total', 1523: 'plant', 1524: 'philosophy', 1525: 'lower', 1526: 'subject', 1527: 'fun', 1528: 'five', 1529: 'event', 1530: 'signs', 1531: 'chicken', 1532: 'recent', 1533: 'aspects', 1534: 'link', 1535: 'restaurant', 1536: 'racist', 1537: 'iii', 1538: 'urine', 1539: 'active', 1540: '40', 1541: 'giving', 1542: 'parts', 1543: 'candidate', 1544: 'benefit', 1545: 'burn', 1546: 'asking', 1547: 'mains', 1548: 'ok', 1549: 'background', 1550: 'performance', 1551: 'quickbooks', 1552: 'drop', 1553: 'cultural', 1554: 'loan', 1555: 'medicine', 1556: 'joke', 1557: 'ipad', 1558: 'package', 1559: 'rbi', 1560: 'moving', 1561: 'original', 1562: 'conversation', 1563: 'branch', 1564: 'pokemon', 1565: 'p', 1566: 'soon', 1567: 'reach', 1568: 'trust', 1569: 'upload', 1570: 'reliable', 1571: 'requirements', 1572: 'films', 1573: 'symptoms', 1574: 'slow', 1575: 'hotels', 1576: 'netflix', 1577: 'fly', 1578: 'clothes', 1579: 'receive', 1580: 'coast', 1581: 'cyrus', 1582: 'grade', 1583: 'navy', 1584: 'race', 1585: 'quantum', 1586: 'france', 1587: 'v', 1588: 'driver', 1589: 'production', 1590: 'dubai', 1591: 'certain', 1592: 'investors', 1593: 'attracted', 1594: 'domain', 1595: 'mistry', 1596: 'naturally', 1597: 'talking', 1598: 'lines', 1599: 'gmat', 1600: 'king', 1601: 'mental', 1602: 'undergraduate', 1603: 'reaction', 1604: 'jews', 1605: 'four', 1606: 'nice', 1607: 'cloud', 1608: 'actors', 1609: 'typical', 1610: 'origin', 1611: 'reliance', 1612: 'banking', 1613: 'promote', 1614: 'wins', 1615: 'blogs', 1616: 'strike', 1617: 'met', 1618: 'unique', 1619: 'marijuana', 1620: '21', 1621: 'achieve', 1622: 'equal', 1623: 'resume', 1624: 'afraid', 1625: 'attractions', 1626: 'meat', 1627: 'west', 1628: 'decide', 1629: \"clinton's\", 1630: 'francisco', 1631: 'essay', 1632: 'somme', 1633: 'serve', 1634: 'came', 1635: 'cutoff', 1636: 'older', 1637: 'learned', 1638: 'son', 1639: 'fluently', 1640: 'effectively', 1641: 'street', 1642: 'horror', 1643: 'return', 1644: 'goes', 1645: 'structures', 1646: 'round', 1647: 'seem', 1648: 'methods', 1649: 'surgery', 1650: 'trade', 1651: 'operating', 1652: 'peace', 1653: 'literature', 1654: 'fact', 1655: 'local', 1656: 'novels', 1657: 'funds', 1658: 'voltage', 1659: 'wordpress', 1660: 'sometimes', 1661: 'comments', 1662: 'knowing', 1663: 'overrated', 1664: 'scam', 1665: 'received', 1666: 'funding', 1667: 'tall', 1668: 'sector', 1669: 'pregnancy', 1670: 'russian', 1671: 'generally', 1672: 'cream', 1673: 'html', 1674: 'motor', 1675: 'placement', 1676: 'africa', 1677: 'debit', 1678: 'rule', 1679: 'moto', 1680: 'pdf', 1681: 'demand', 1682: 'caused', 1683: 'union', 1684: 'characters', 1685: 'told', 1686: 'hide', 1687: 'hacked', 1688: 'grades', 1689: 'sense', 1690: \"modi's\", 1691: 'fiction', 1692: 'measure', 1693: 'regular', 1694: 'selling', 1695: 'novel', 1696: 'concept', 1697: 'died', 1698: 'spoken', 1699: 'visitors', 1700: 'symbol', 1701: 'rock', 1702: 'feelings', 1703: 'switch', 1704: 'architecture', 1705: 'notice', 1706: 'co', 1707: 'hiring', 1708: 'wave', 1709: 'lives', 1710: 'colors', 1711: 'ii', 1712: 'weakest', 1713: 'results', 1714: 'cycle', 1715: '3d', 1716: 'organic', 1717: 'minutes', 1718: 'feed', 1719: 'pre', 1720: 'led', 1721: 'empire', 1722: 'images', 1723: 'hit', 1724: 'produce', 1725: 'define', 1726: 'phase', 1727: '6s', 1728: 'released', 1729: 'focus', 1730: 'formed', 1731: 'wake', 1732: 'treated', 1733: 'paying', 1734: '15000', 1735: 'truth', 1736: 'api', 1737: 'species', 1738: 'half', 1739: 'brown', 1740: 'skill', 1741: 'stupid', 1742: 'chrome', 1743: 'cover', 1744: 'sentences', 1745: 'gandhi', 1746: 'steel', 1747: 'usually', 1748: 'calls', 1749: 'purchase', 1750: 'eggs', 1751: 'calories', 1752: 'kolkata', 1753: 'identify', 1754: 'mistake', 1755: 'metal', 1756: 'extra', 1757: 'goal', 1758: 'loves', 1759: 'similarities', 1760: 'technologies', 1761: 'batman', 1762: 'hear', 1763: 'upgrade', 1764: 'rates', 1765: 'sa', 1766: 'gun', 1767: 'excel', 1768: 'analyst', 1769: 'lyrics', 1770: 'citizen', 1771: 'glass', 1772: 'presence', 1773: 'ibps', 1774: 'wedding', 1775: 'base', 1776: 'bit', 1777: 'removed', 1778: 'mars', 1779: 'usb', 1780: 'campaign', 1781: 'payment', 1782: 'sugar', 1783: 'reviews', 1784: 'built', 1785: 'ac', 1786: 'taller', 1787: 'fit', 1788: 'entire', 1789: 'various', 1790: 'experienced', 1791: 'handle', 1792: 'electricity', 1793: 'style', 1794: 'exchange', 1795: 'fail', 1796: 'error', 1797: 'killed', 1798: 'permanently', 1799: 'materials', 1800: 'pilot', 1801: 'algorithm', 1802: 'virtual', 1803: 'easier', 1804: 'christmas', 1805: 'naruto', 1806: 'failure', 1807: 'cannot', 1808: 'actress', 1809: 'xbox', 1810: 'gravitational', 1811: 'solutions', 1812: 'unknown', 1813: 'depressed', 1814: 'scratch', 1815: 'accurate', 1816: 'democracy', 1817: 'linkedin', 1818: 'certification', 1819: 'probability', 1820: 'bar', 1821: 'wait', 1822: 'speaker', 1823: 'became', 1824: 'iim', 1825: 'introvert', 1826: 'shot', 1827: 'airport', 1828: 'anybody', 1829: 'fi', 1830: 'dhoni', 1831: 'launch', 1832: 'truly', 1833: 'supply', 1834: 'valley', 1835: 'groups', 1836: 'plane', 1837: 'tinder', 1838: 'worse', 1839: 'dry', 1840: 'community', 1841: 'changes', 1842: 'emails', 1843: 'laptops', 1844: 'hold', 1845: 'lie', 1846: 'artificial', 1847: 'played', 1848: 'result', 1849: 'alive', 1850: 'regarding', 1851: 'anymore', 1852: 'prices', 1853: 'box', 1854: 'harmful', 1855: 'alternative', 1856: \"shouldn't\", 1857: 'bed', 1858: 'applying', 1859: 'theories', 1860: 'biology', 1861: 'temple', 1862: 'sad', 1863: 'scale', 1864: 'networking', 1865: 'exercises', 1866: 'along', 1867: 'mutual', 1868: 'gender', 1869: 'six', 1870: 'js', 1871: 'placements', 1872: 'smoke', 1873: 'evil', 1874: 'raise', 1875: 'scientifically', 1876: 'suddenly', 1877: 'tricks', 1878: 'hardest', 1879: 'plants', 1880: 'redmi', 1881: 'colour', 1882: 'atheists', 1883: 'carbon', 1884: 'activities', 1885: 'especially', 1886: 'shall', 1887: 'forces', 1888: 'register', 1889: 'developing', 1890: 'report', 1891: 'nose', 1892: 'offline', 1893: 'connection', 1894: 'fish', 1895: 'rules', 1896: 'areas', 1897: 'constitution', 1898: 'rent', 1899: 'road', 1900: 'print', 1901: 'intel', 1902: 'occur', 1903: 'largest', 1904: 'ece', 1905: 'entrance', 1906: 'kejriwal', 1907: 'piece', 1908: 'count', 1909: 'equivalent', 1910: 'zone', 1911: 'ip', 1912: '200', 1913: 'legally', 1914: 'respond', 1915: 'named', 1916: 'pick', 1917: 'beat', 1918: \"one's\", 1919: 'piano', 1920: 'professor', 1921: 'evolution', 1922: 'situation', 1923: 'revenue', 1924: 'efficient', 1925: 'suggestions', 1926: 'stage', 1927: 'freedom', 1928: 'catch', 1929: 'employee', 1930: 'christian', 1931: 'almost', 1932: 'consequences', 1933: 'visitor', 1934: 'basis', 1935: 'scrapping', 1936: 'pride', 1937: 'un', 1938: 'members', 1939: 'players', 1940: 'raw', 1941: 'invited', 1942: 'atheist', 1943: 'prison', 1944: 'dna', 1945: 'length', 1946: 'designing', 1947: 'hands', 1948: 'democratic', 1949: 'central', 1950: 'asia', 1951: 'besides', 1952: 'spouse', 1953: 'weather', 1954: 'century', 1955: 'strikes', 1956: 'gobi', 1957: 'mistakes', 1958: 'o', 1959: 'salt', 1960: 'developers', 1961: 'statement', 1962: 'feet', 1963: 'action', 1964: 'individuals', 1965: 'pronunciation', 1966: 'affordable', 1967: 'lewis', 1968: 'fashion', 1969: 'dress', 1970: '15k', 1971: 'involved', 1972: 'itself', 1973: 'daughter', 1974: 'iron', 1975: 'environment', 1976: 'tickets', 1977: 'joining', 1978: 'limited', 1979: 'framework', 1980: 'router', 1981: 'ad', 1982: 'ads', 1983: 'frac', 1984: 'liquid', 1985: 'virus', 1986: 'canadian', 1987: 'stopped', 1988: 'loose', 1989: 'entrepreneur', 1990: 'unlock', 1991: 'venture', 1992: 'density', 1993: 'masturbate', 1994: 'entry', 1995: 'brands', 1996: 'scene', 1997: 'ideal', 1998: 'industrial', 1999: 'faux', 2000: 'pas'}\n",
            "What is the step by step guide to invest in share market in india?\n",
            "[3, 4, 2, 1236, 59, 1236, 2001, 8, 572, 9, 774, 371, 9, 36]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    3    4    2\n",
            " 1236   59 1236 2001    8  572    9  774  371    9   36]\n",
            "2001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj9YRhzSKoDk",
        "colab_type": "code",
        "outputId": "afaa1871-6fcf-431f-ad9c-58e726903fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "glove_embedding_matrix = np.zeros((NB_WORDS+1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i < NB_WORDS+1: #+1 for 'unk' oov token\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            glove_embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            # words not found in embedding index will the word embedding of unk\n",
        "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Null word embeddings: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SxzWUi96gKX",
        "colab_type": "code",
        "outputId": "38cb0463-40bd-467d-e9ec-09b264a39078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "print(embeddings_index['unk'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.0071e-01 -4.6867e-01 -2.0617e-01 -8.0978e-01 -2.3889e-01  2.4329e-01\n",
            "  1.6538e-02 -3.5687e-02 -2.2306e-01  9.5189e-01 -3.2273e-01  2.1980e-01\n",
            " -6.7524e-02 -3.7220e-01 -3.9718e-01 -4.3861e-01  1.1967e-01 -2.9964e-01\n",
            "  2.8437e-02 -8.7544e-02  1.6569e-01 -4.9451e-01 -6.2011e-01 -1.6574e-01\n",
            " -9.7218e-02 -9.9474e-02 -8.0307e-02 -3.9338e-01 -2.4195e-01  3.2023e-01\n",
            " -5.3320e-01 -4.0184e-01 -6.7135e-01 -7.8561e-02  5.5546e-01  2.9997e-01\n",
            " -9.9650e-02 -6.7035e-01  1.2669e-01 -1.8618e-01 -6.2621e-02  4.5290e-01\n",
            "  3.9265e-01  2.4121e-01 -4.1474e-01 -6.1890e-01 -1.0412e-01 -3.1043e-01\n",
            " -6.6788e-03 -8.3248e-01  6.5150e-01  9.0181e-01  2.4146e-02 -7.0766e-02\n",
            " -3.9580e-01 -3.6487e-01 -2.3929e-01 -1.5145e-01  2.0777e-01  5.4671e-01\n",
            " -2.5042e-01 -6.0142e-01 -5.4820e-01  7.7249e-03 -5.3288e-01  5.0325e-01\n",
            " -1.2712e-01  1.1989e-01 -6.4584e-01  3.5576e-01  1.7496e-01  1.1838e-01\n",
            " -3.2181e-01  7.4814e-02 -9.0381e-02 -2.9843e-01  1.6798e-02 -1.2735e-01\n",
            "  7.3567e-01 -1.7335e-01  3.7123e-01  3.7979e-01 -5.1801e-01  2.7621e-01\n",
            "  2.1512e-01 -8.2588e-02  2.1638e-01  1.2595e-01  3.8436e-01 -1.3332e-01\n",
            "  5.7185e-02 -2.8127e-01 -4.4310e-01  1.3498e-01  1.3306e-01 -3.2050e-02\n",
            "  1.9719e-01  2.5455e-01  6.3475e-01 -2.3474e-01 -3.6038e-01  4.1148e-02\n",
            " -2.4422e-01  8.3731e-01 -2.2504e-01 -2.9683e-01  6.3898e-01 -3.9774e-01\n",
            " -1.0322e-01 -1.7446e-01 -7.8059e-02  2.6479e-01 -4.2250e-01 -1.0671e-01\n",
            " -9.6468e-02 -1.7027e-01  2.7497e-01 -1.2813e-01  2.4751e-01  2.5999e-01\n",
            "  1.8327e-01  1.0988e-01  3.5486e-04 -4.9029e-01  1.9582e-01 -4.5226e-01\n",
            " -1.3617e-02  1.0765e-01 -1.6161e-02 -2.7242e-01  7.7877e-02 -1.1860e-01\n",
            "  9.2792e-02 -4.3774e-01 -2.6539e-01 -2.6590e-01  8.0585e-02  1.8626e-01\n",
            "  1.7362e-01 -2.0242e-01  3.5327e-01 -6.4335e-02  1.3764e-01 -4.4417e-01\n",
            "  8.7521e-01 -2.3260e-01 -6.7657e-01  2.3891e-01 -8.0176e-02  4.9526e-01\n",
            " -2.8579e-01  2.5041e-01  1.5853e-01 -1.2960e-01 -5.1529e-01 -3.3175e-01\n",
            "  4.1826e-01  3.3211e-01 -1.1793e+00  2.2818e-01 -5.7755e-01  7.7314e-01\n",
            "  1.6093e-01  2.3360e-01 -1.8764e-01 -2.4516e-01 -5.4803e-01  2.3110e-01\n",
            " -3.2975e-01 -1.2646e-01  3.7984e-01  3.6006e-01  6.0382e-01 -1.5882e-01\n",
            " -4.3682e-01 -6.3444e-01 -2.8830e-01 -1.3609e-01 -2.5821e-02 -4.0767e-01\n",
            "  1.8636e-01 -4.5857e-01 -2.4611e-01 -4.5890e-02  8.7613e-02 -1.5685e-01\n",
            "  3.0129e-01 -8.0176e-01  1.2363e-01  7.1458e-03  1.4751e-01  3.5471e-01\n",
            "  1.5120e-01  8.1938e-02 -3.6711e-01 -2.7208e-01 -3.5597e-01  1.7207e-01\n",
            " -4.1850e-02  4.9547e-01 -3.1630e-01  3.4915e-01 -3.7295e-02  2.0996e-01\n",
            " -3.0103e-01 -1.0875e-01  3.0354e-01  2.8157e-01 -7.9880e-02 -5.0611e-01\n",
            " -2.9416e-01 -3.0861e-01 -8.2462e-01 -1.0019e-01  9.0473e-02 -1.8238e-01\n",
            " -5.9223e-03  6.3833e-02  1.5210e-01 -2.5385e-01 -6.8831e-01 -3.4549e-02\n",
            "  4.5180e-01  6.2293e-02 -4.6343e-01  4.0400e-01  4.5106e-02  1.7375e-01\n",
            " -2.7745e-02  3.6361e-01  7.8235e-02  1.9538e-01 -1.6506e-01  3.9627e-02\n",
            " -3.0113e-01  2.2257e-01  2.6773e-02  1.9151e-01  4.9987e-01 -3.5491e-01\n",
            " -1.9928e-02  9.0701e-01 -8.5490e-01 -3.9361e-01  4.1030e-01  1.4631e-01\n",
            " -1.5664e-01  5.3971e-01  1.4869e-01 -5.5193e-02 -4.7718e-01 -3.7498e-01\n",
            " -5.3233e-01  1.2320e-01 -1.2227e-01 -5.8157e-02  2.4245e-02  1.2577e-01\n",
            " -1.8558e-01  7.5054e-02 -7.2822e-01 -3.5878e-02  1.5989e-01 -2.5743e-01\n",
            "  2.1856e-01 -2.2810e-01  4.8356e-02  5.6023e-02  1.5111e-01  2.2945e-01\n",
            "  4.5846e-01 -8.7056e-02 -2.9662e-01  1.5054e-02  2.1102e-01 -3.7446e-02\n",
            "  7.8902e-01 -3.4193e-01 -8.2430e-01 -7.1748e-01 -1.9649e-01  8.7570e-02\n",
            " -2.0552e-01  1.4610e-02 -3.8088e-01  6.5309e-01 -2.0561e-01 -2.8427e-02\n",
            "  1.0577e-02  8.8410e-03  1.1992e-01  1.4611e-01  1.6034e-01  7.2431e-02\n",
            " -4.3760e-01 -2.5979e-01  5.8158e-01  4.9267e-01 -1.1276e-01 -2.7775e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HTbmZTPLAfW",
        "colab_type": "code",
        "outputId": "04a9456d-a16a-4413-80c8-e09608f12fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 100\n",
        "max_len = MAX_SEQUENCE_LENGTH\n",
        "emb_dim = EMBEDDING_DIM\n",
        "latent_dim = 64\n",
        "intermediate_dim = 256\n",
        "epsilon_std = 1.0\n",
        "kl_weight = 100         #CHANGE THIS\n",
        "num_sampled=500\n",
        "act = ELU()\n",
        "\n",
        "\n",
        "x = Input(shape=(max_len,))\n",
        "x_embed = Embedding(NB_WORDS+1, emb_dim, weights=[glove_embedding_matrix],\n",
        "                            input_length=max_len, trainable=False)(x)\n",
        "h = Bidirectional(CuDNNLSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(x_embed)\n",
        "\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "# we instantiate these layers separately so as to reuse them later\n",
        "repeated_context = RepeatVector(max_len)\n",
        "decoder_h = CuDNNLSTM(intermediate_dim, return_sequences=True)\n",
        "decoder_mean = Dense(NB_WORDS+1, activation='linear') #softmax is applied in the seq2seqloss by tf #TimeDistributed()\n",
        "h_decoded = decoder_h(repeated_context(z))\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "\n",
        "# placeholder loss\n",
        "def zero_loss(y_true, y_pred):\n",
        "    return K.zeros_like(y_pred)\n",
        "\n",
        "class CustomVariationalLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
        "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
        "\n",
        "    def vae_loss(self, x, x_decoded_mean):\n",
        "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
        "        labels = tf.cast(x, tf.int32)\n",
        "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
        "                                                     weights=self.target_weights,\n",
        "                                                     average_across_timesteps=False,\n",
        "                                                     average_across_batch=False), axis=-1)#,\n",
        "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n",
        "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        xent_loss = K.mean(xent_loss)\n",
        "        kl_loss = K.mean(kl_loss)\n",
        "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        x_decoded_mean = inputs[1]\n",
        "        print(x.shape, x_decoded_mean.shape)\n",
        "        loss = self.vae_loss(x, x_decoded_mean)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # we don't use this output, but it has to have the correct shape:\n",
        "        return K.ones_like(x)\n",
        "    \n",
        "def kl_loss(x, x_decoded_mean):\n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    kl_loss = kl_weight * kl_loss\n",
        "    return kl_loss\n",
        "\n",
        "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
        "vae = Model(x, [loss_layer])\n",
        "opt = Adam(lr=0.01) \n",
        "vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])\n",
        "vae.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "(?, 25) (100, 25, 2002)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 25, 300)      600600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 512)          1142784     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 64)           0           dense_1[0][0]                    \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 25, 64)       0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)        (None, 25, 256)      329728      repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 25, 2002)     514514      cu_dnnlstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer_1 (Cus [(None, 25), (None,  0           input_1[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,653,290\n",
            "Trainable params: 2,052,690\n",
            "Non-trainable params: 600,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLBOcCMYF5g9",
        "colab_type": "code",
        "outputId": "4b626c9f-b205-4b89-ed7a-f4ff7c835266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def create_model_checkpoint(dir, model_name):\n",
        "    filepath = dir + '/' + model_name + \".h5\" \n",
        "    directory = os.path.dirname(filepath)\n",
        "    try:\n",
        "        os.stat(directory)\n",
        "    except:\n",
        "        os.mkdir(directory)\n",
        "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=2, save_best_only=True)\n",
        "    return checkpointer\n",
        "\n",
        "checkpointer = create_model_checkpoint('drive/My Drive/models_250', 'vae_seq2seq_test_very_high_std')\n",
        "\n",
        "\n",
        "\n",
        "vae.fit(data_train, data_train,\n",
        "     shuffle=True,\n",
        "     epochs=100,\n",
        "     batch_size=batch_size,\n",
        "     verbose = 2,\n",
        "     validation_data=(data_val, data_val), callbacks=[checkpointer])\n",
        "\n",
        "#print(K.eval(vae.optimizer.lr))\n",
        "#K.set_value(vae.optimizer.lr, 0.01)\n",
        "\n",
        "vae.save('drive/My Drive/models_250/vae_lstm.h5')\n",
        "#vae.load_weights('models/vae_seq2seq_test.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 775000 samples, validate on 8000 samples\n",
            "Epoch 1/100\n",
            " - 118s - loss: 61.8962 - kl_loss: 0.0049 - val_loss: 62.1455 - val_kl_loss: 0.0013\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 62.14545, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 2/100\n",
            " - 118s - loss: 61.5816 - kl_loss: 0.0052 - val_loss: 62.0417 - val_kl_loss: 6.0842e-04\n",
            "\n",
            "Epoch 00002: val_loss improved from 62.14545 to 62.04171, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 3/100\n",
            " - 118s - loss: 61.5199 - kl_loss: 0.0117 - val_loss: 62.0047 - val_kl_loss: 4.0861e-04\n",
            "\n",
            "Epoch 00003: val_loss improved from 62.04171 to 62.00474, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 4/100\n",
            " - 118s - loss: 61.4676 - kl_loss: 0.0016 - val_loss: 61.9890 - val_kl_loss: 2.6667e-04\n",
            "\n",
            "Epoch 00004: val_loss improved from 62.00474 to 61.98897, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 5/100\n",
            " - 118s - loss: 61.4381 - kl_loss: 9.1146e-04 - val_loss: 61.9498 - val_kl_loss: 2.0410e-04\n",
            "\n",
            "Epoch 00005: val_loss improved from 61.98897 to 61.94980, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 6/100\n",
            " - 118s - loss: 61.4158 - kl_loss: 4.9660e-04 - val_loss: 61.9347 - val_kl_loss: 0.0015\n",
            "\n",
            "Epoch 00006: val_loss improved from 61.94980 to 61.93472, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 7/100\n",
            " - 119s - loss: 61.3999 - kl_loss: 5.1962e-04 - val_loss: 61.9111 - val_kl_loss: 1.0848e-04\n",
            "\n",
            "Epoch 00007: val_loss improved from 61.93472 to 61.91106, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 8/100\n",
            " - 119s - loss: 61.3923 - kl_loss: 5.3854e-04 - val_loss: 61.9145 - val_kl_loss: 3.4390e-04\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 61.91106\n",
            "Epoch 9/100\n",
            " - 119s - loss: 61.3763 - kl_loss: 4.1123e-04 - val_loss: 61.9142 - val_kl_loss: 3.7760e-04\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 61.91106\n",
            "Epoch 10/100\n",
            " - 119s - loss: 61.3677 - kl_loss: 2.1554e-04 - val_loss: 61.8946 - val_kl_loss: 1.6792e-04\n",
            "\n",
            "Epoch 00010: val_loss improved from 61.91106 to 61.89463, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 11/100\n",
            " - 119s - loss: 61.3608 - kl_loss: 5.0752e-04 - val_loss: 61.9005 - val_kl_loss: 6.5884e-05\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 61.89463\n",
            "Epoch 12/100\n",
            " - 120s - loss: 61.3530 - kl_loss: 1.0217e-04 - val_loss: 61.8954 - val_kl_loss: 8.8215e-05\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 61.89463\n",
            "Epoch 13/100\n",
            " - 120s - loss: 61.3524 - kl_loss: 3.7230e-04 - val_loss: 61.8849 - val_kl_loss: 7.4854e-05\n",
            "\n",
            "Epoch 00013: val_loss improved from 61.89463 to 61.88486, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 14/100\n",
            " - 120s - loss: 61.3462 - kl_loss: 7.8902e-05 - val_loss: 61.8843 - val_kl_loss: 8.8929e-05\n",
            "\n",
            "Epoch 00014: val_loss improved from 61.88486 to 61.88434, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 15/100\n",
            " - 119s - loss: 61.3455 - kl_loss: 1.8154e-04 - val_loss: 61.8835 - val_kl_loss: 5.3303e-05\n",
            "\n",
            "Epoch 00015: val_loss improved from 61.88434 to 61.88354, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 16/100\n",
            " - 119s - loss: 61.3393 - kl_loss: 5.7861e-05 - val_loss: 61.8806 - val_kl_loss: 8.1136e-05\n",
            "\n",
            "Epoch 00016: val_loss improved from 61.88354 to 61.88063, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 17/100\n",
            " - 119s - loss: 61.3383 - kl_loss: 5.9691e-05 - val_loss: 61.8713 - val_kl_loss: 5.0066e-05\n",
            "\n",
            "Epoch 00017: val_loss improved from 61.88063 to 61.87126, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 18/100\n",
            " - 119s - loss: 61.3366 - kl_loss: 4.1519e-05 - val_loss: 61.8939 - val_kl_loss: 2.6434e-05\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 61.87126\n",
            "Epoch 19/100\n",
            " - 120s - loss: 61.3355 - kl_loss: 4.9346e-05 - val_loss: 61.8895 - val_kl_loss: 2.7215e-05\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 61.87126\n",
            "Epoch 20/100\n",
            " - 120s - loss: 61.3340 - kl_loss: 4.3066e-05 - val_loss: 61.8779 - val_kl_loss: 2.8755e-05\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 61.87126\n",
            "Epoch 21/100\n",
            " - 120s - loss: 61.3315 - kl_loss: 3.9435e-05 - val_loss: 61.8885 - val_kl_loss: 2.4460e-05\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 61.87126\n",
            "Epoch 22/100\n",
            " - 119s - loss: 61.3306 - kl_loss: 1.1638e-04 - val_loss: 61.8744 - val_kl_loss: 3.1392e-05\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 61.87126\n",
            "Epoch 23/100\n",
            " - 119s - loss: 61.3290 - kl_loss: 7.5367e-05 - val_loss: 61.8907 - val_kl_loss: 2.4033e-05\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 61.87126\n",
            "Epoch 24/100\n",
            " - 120s - loss: 61.3277 - kl_loss: 4.1942e-05 - val_loss: 61.8819 - val_kl_loss: 5.9565e-05\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 61.87126\n",
            "Epoch 25/100\n",
            " - 120s - loss: 61.3272 - kl_loss: 5.7816e-05 - val_loss: 61.8755 - val_kl_loss: 5.7276e-05\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 61.87126\n",
            "Epoch 26/100\n",
            " - 120s - loss: 61.3254 - kl_loss: 5.8978e-05 - val_loss: 61.8873 - val_kl_loss: 7.4705e-05\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 61.87126\n",
            "Epoch 27/100\n",
            " - 120s - loss: 61.3241 - kl_loss: 4.9733e-05 - val_loss: 61.8715 - val_kl_loss: 2.8779e-05\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 61.87126\n",
            "Epoch 28/100\n",
            " - 120s - loss: 61.3232 - kl_loss: 4.9562e-05 - val_loss: 61.8737 - val_kl_loss: 2.9914e-05\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 61.87126\n",
            "Epoch 29/100\n",
            " - 119s - loss: 61.3227 - kl_loss: 1.1218e-04 - val_loss: 61.8800 - val_kl_loss: 3.1297e-05\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 61.87126\n",
            "Epoch 30/100\n",
            " - 119s - loss: 61.3222 - kl_loss: 4.8625e-05 - val_loss: 61.8769 - val_kl_loss: 3.1107e-05\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 61.87126\n",
            "Epoch 31/100\n",
            " - 119s - loss: 61.3205 - kl_loss: 4.3818e-05 - val_loss: 61.8767 - val_kl_loss: 6.5898e-05\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 61.87126\n",
            "Epoch 32/100\n",
            " - 119s - loss: 61.3304 - kl_loss: 0.0047 - val_loss: 61.8653 - val_kl_loss: 3.6614e-05\n",
            "\n",
            "Epoch 00032: val_loss improved from 61.87126 to 61.86534, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 33/100\n",
            " - 119s - loss: 61.3206 - kl_loss: 7.8899e-05 - val_loss: 61.8729 - val_kl_loss: 5.2800e-05\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 61.86534\n",
            "Epoch 34/100\n",
            " - 119s - loss: 61.3200 - kl_loss: 8.9028e-05 - val_loss: 61.8686 - val_kl_loss: 5.5389e-05\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 61.86534\n",
            "Epoch 35/100\n",
            " - 119s - loss: 61.3193 - kl_loss: 7.4919e-05 - val_loss: 61.8675 - val_kl_loss: 2.2436e-05\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 61.86534\n",
            "Epoch 36/100\n",
            " - 119s - loss: 61.3219 - kl_loss: 2.4004e-04 - val_loss: 61.8802 - val_kl_loss: 2.1808e-05\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 61.86534\n",
            "Epoch 37/100\n",
            " - 119s - loss: 61.3191 - kl_loss: 5.0666e-05 - val_loss: 61.8662 - val_kl_loss: 4.4228e-05\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 61.86534\n",
            "Epoch 38/100\n",
            " - 119s - loss: 61.3186 - kl_loss: 1.3292e-04 - val_loss: 61.8684 - val_kl_loss: 4.9629e-05\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 61.86534\n",
            "Epoch 39/100\n",
            " - 119s - loss: 61.3184 - kl_loss: 6.5673e-05 - val_loss: 61.8694 - val_kl_loss: 5.8848e-05\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 61.86534\n",
            "Epoch 40/100\n",
            " - 119s - loss: 61.3178 - kl_loss: 6.8656e-05 - val_loss: 61.8710 - val_kl_loss: 4.6893e-05\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 61.86534\n",
            "Epoch 41/100\n",
            " - 119s - loss: 61.3166 - kl_loss: 8.3257e-05 - val_loss: 61.8616 - val_kl_loss: 2.8037e-05\n",
            "\n",
            "Epoch 00041: val_loss improved from 61.86534 to 61.86158, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 42/100\n",
            " - 119s - loss: 61.3172 - kl_loss: 7.8193e-05 - val_loss: 61.8709 - val_kl_loss: 9.8740e-05\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 61.86158\n",
            "Epoch 43/100\n",
            " - 119s - loss: 61.3162 - kl_loss: 6.1028e-05 - val_loss: 61.8661 - val_kl_loss: 2.6720e-05\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 61.86158\n",
            "Epoch 44/100\n",
            " - 120s - loss: 61.3164 - kl_loss: 5.8379e-05 - val_loss: 61.8737 - val_kl_loss: 3.0199e-05\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 61.86158\n",
            "Epoch 45/100\n",
            " - 119s - loss: 61.3204 - kl_loss: 1.2332e-04 - val_loss: 61.8681 - val_kl_loss: 1.8731e-05\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 61.86158\n",
            "Epoch 46/100\n",
            " - 119s - loss: 61.3173 - kl_loss: 9.3113e-05 - val_loss: 61.8663 - val_kl_loss: 6.1103e-05\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 61.86158\n",
            "Epoch 47/100\n",
            " - 119s - loss: 61.3159 - kl_loss: 7.8302e-05 - val_loss: 61.8698 - val_kl_loss: 3.7778e-05\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 61.86158\n",
            "Epoch 48/100\n",
            " - 119s - loss: 61.3152 - kl_loss: 1.0989e-04 - val_loss: 61.8665 - val_kl_loss: 8.7939e-05\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 61.86158\n",
            "Epoch 49/100\n",
            " - 119s - loss: 61.3148 - kl_loss: 6.6774e-05 - val_loss: 61.8649 - val_kl_loss: 3.6697e-05\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 61.86158\n",
            "Epoch 50/100\n",
            " - 119s - loss: 61.3148 - kl_loss: 8.4567e-05 - val_loss: 61.8761 - val_kl_loss: 6.7281e-05\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 61.86158\n",
            "Epoch 51/100\n",
            " - 119s - loss: 61.3145 - kl_loss: 7.6724e-05 - val_loss: 61.8725 - val_kl_loss: 4.3872e-05\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 61.86158\n",
            "Epoch 52/100\n",
            " - 119s - loss: 61.3142 - kl_loss: 6.6709e-05 - val_loss: 61.8839 - val_kl_loss: 1.9620e-04\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 61.86158\n",
            "Epoch 53/100\n",
            " - 119s - loss: 61.3237 - kl_loss: 5.5633e-04 - val_loss: 61.8669 - val_kl_loss: 7.5458e-05\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 61.86158\n",
            "Epoch 54/100\n",
            " - 119s - loss: 61.3145 - kl_loss: 7.2101e-05 - val_loss: 61.8696 - val_kl_loss: 2.0547e-04\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 61.86158\n",
            "Epoch 55/100\n",
            " - 119s - loss: 61.3146 - kl_loss: 8.6436e-05 - val_loss: 61.8782 - val_kl_loss: 3.5815e-05\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 61.86158\n",
            "Epoch 56/100\n",
            " - 119s - loss: 61.3152 - kl_loss: 5.6123e-04 - val_loss: 61.8721 - val_kl_loss: 4.5612e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 61.86158\n",
            "Epoch 57/100\n",
            " - 119s - loss: 61.3146 - kl_loss: 8.9403e-05 - val_loss: 61.8647 - val_kl_loss: 2.9509e-05\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 61.86158\n",
            "Epoch 58/100\n",
            " - 118s - loss: 61.3146 - kl_loss: 9.0884e-05 - val_loss: 61.8685 - val_kl_loss: 3.5920e-05\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 61.86158\n",
            "Epoch 59/100\n",
            " - 118s - loss: 61.3142 - kl_loss: 1.0270e-04 - val_loss: 61.8824 - val_kl_loss: 6.9670e-05\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 61.86158\n",
            "Epoch 60/100\n",
            " - 119s - loss: 61.3131 - kl_loss: 8.4905e-05 - val_loss: 61.8596 - val_kl_loss: 3.3545e-05\n",
            "\n",
            "Epoch 00060: val_loss improved from 61.86158 to 61.85958, saving model to drive/My Drive/models_250/vae_seq2seq_test_very_high_std.h5\n",
            "Epoch 61/100\n",
            " - 120s - loss: 61.3133 - kl_loss: 1.0513e-04 - val_loss: 61.8655 - val_kl_loss: 5.3496e-05\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 61.85958\n",
            "Epoch 62/100\n",
            " - 120s - loss: 61.3134 - kl_loss: 1.1014e-04 - val_loss: 61.8687 - val_kl_loss: 9.6983e-05\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 61.85958\n",
            "Epoch 63/100\n",
            " - 119s - loss: 61.3426 - kl_loss: 0.0162 - val_loss: 61.8932 - val_kl_loss: 9.1412e-04\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 61.85958\n",
            "Epoch 64/100\n",
            " - 119s - loss: 61.3198 - kl_loss: 6.6962e-04 - val_loss: 61.8970 - val_kl_loss: 5.3220e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 61.85958\n",
            "Epoch 65/100\n",
            " - 119s - loss: 61.3136 - kl_loss: 2.9214e-04 - val_loss: 61.8714 - val_kl_loss: 7.2144e-05\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 61.85958\n",
            "Epoch 66/100\n",
            " - 119s - loss: 61.3134 - kl_loss: 1.3277e-04 - val_loss: 61.8894 - val_kl_loss: 7.8633e-04\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 61.85958\n",
            "Epoch 67/100\n",
            " - 119s - loss: 61.3228 - kl_loss: 0.0033 - val_loss: 61.8630 - val_kl_loss: 1.6188e-04\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 61.85958\n",
            "Epoch 68/100\n",
            " - 119s - loss: 61.3121 - kl_loss: 1.3032e-04 - val_loss: 61.8712 - val_kl_loss: 4.4603e-04\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 61.85958\n",
            "Epoch 69/100\n",
            " - 119s - loss: 61.3117 - kl_loss: 1.1535e-04 - val_loss: 61.8692 - val_kl_loss: 3.3591e-05\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 61.85958\n",
            "Epoch 70/100\n",
            " - 119s - loss: 61.3200 - kl_loss: 0.0022 - val_loss: 61.8862 - val_kl_loss: 4.7236e-05\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 61.85958\n",
            "Epoch 71/100\n",
            " - 119s - loss: 61.3184 - kl_loss: 1.2803e-04 - val_loss: 61.8691 - val_kl_loss: 1.9072e-04\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 61.85958\n",
            "Epoch 72/100\n",
            " - 119s - loss: 61.3127 - kl_loss: 2.3233e-04 - val_loss: 61.8648 - val_kl_loss: 2.9280e-05\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 61.85958\n",
            "Epoch 73/100\n",
            " - 119s - loss: 61.3146 - kl_loss: 4.2210e-04 - val_loss: 61.8716 - val_kl_loss: 2.7093e-05\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 61.85958\n",
            "Epoch 74/100\n",
            " - 119s - loss: 61.3119 - kl_loss: 1.0107e-04 - val_loss: 61.8639 - val_kl_loss: 3.5511e-05\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 61.85958\n",
            "Epoch 75/100\n",
            " - 119s - loss: 61.3333 - kl_loss: 0.0012 - val_loss: 61.8671 - val_kl_loss: 2.8798e-05\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 61.85958\n",
            "Epoch 76/100\n",
            " - 119s - loss: 61.3679 - kl_loss: 0.0019 - val_loss: 61.9509 - val_kl_loss: 1.9564e-04\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 61.85958\n",
            "Epoch 77/100\n",
            " - 120s - loss: 61.6840 - kl_loss: 0.0351 - val_loss: 62.8417 - val_kl_loss: 0.0037\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 61.85958\n",
            "Epoch 78/100\n",
            " - 120s - loss: 61.7269 - kl_loss: 0.0066 - val_loss: 61.9285 - val_kl_loss: 2.9055e-04\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 61.85958\n",
            "Epoch 79/100\n",
            " - 120s - loss: 61.3426 - kl_loss: 0.0012 - val_loss: 61.8693 - val_kl_loss: 3.3088e-05\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 61.85958\n",
            "Epoch 80/100\n",
            " - 120s - loss: 61.3159 - kl_loss: 1.3220e-04 - val_loss: 61.8636 - val_kl_loss: 0.0016\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 61.85958\n",
            "Epoch 81/100\n",
            " - 119s - loss: 61.3145 - kl_loss: 1.6912e-04 - val_loss: 61.8679 - val_kl_loss: 1.7593e-04\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 61.85958\n",
            "Epoch 82/100\n",
            " - 119s - loss: 61.3131 - kl_loss: 1.7386e-04 - val_loss: 61.8604 - val_kl_loss: 1.5818e-04\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 61.85958\n",
            "Epoch 83/100\n",
            " - 119s - loss: 61.3186 - kl_loss: 6.4847e-04 - val_loss: 61.8811 - val_kl_loss: 3.1080e-05\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 61.85958\n",
            "Epoch 84/100\n",
            " - 119s - loss: 61.3156 - kl_loss: 1.7912e-04 - val_loss: 61.8660 - val_kl_loss: 4.0116e-05\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 61.85958\n",
            "Epoch 85/100\n",
            " - 119s - loss: 61.3152 - kl_loss: 5.4987e-04 - val_loss: 61.9129 - val_kl_loss: 5.9206e-04\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 61.85958\n",
            "Epoch 86/100\n",
            " - 119s - loss: 61.3921 - kl_loss: 0.0041 - val_loss: 61.8914 - val_kl_loss: 9.5244e-04\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 61.85958\n",
            "Epoch 87/100\n",
            " - 119s - loss: 61.3401 - kl_loss: 0.0053 - val_loss: 61.8795 - val_kl_loss: 6.1602e-05\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 61.85958\n",
            "Epoch 88/100\n",
            " - 120s - loss: 61.3135 - kl_loss: 7.9949e-05 - val_loss: 61.8619 - val_kl_loss: 3.6713e-05\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 61.85958\n",
            "Epoch 89/100\n",
            " - 120s - loss: 61.3125 - kl_loss: 1.7495e-04 - val_loss: 61.8647 - val_kl_loss: 5.6170e-05\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 61.85958\n",
            "Epoch 90/100\n",
            " - 120s - loss: 61.3121 - kl_loss: 1.4327e-04 - val_loss: 61.8622 - val_kl_loss: 3.7453e-05\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 61.85958\n",
            "Epoch 91/100\n",
            " - 120s - loss: 61.3223 - kl_loss: 0.0048 - val_loss: 62.0128 - val_kl_loss: 0.0748\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 61.85958\n",
            "Epoch 92/100\n",
            " - 119s - loss: 61.3878 - kl_loss: 0.0053 - val_loss: 61.8927 - val_kl_loss: 7.4109e-05\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 61.85958\n",
            "Epoch 93/100\n",
            " - 119s - loss: 61.3489 - kl_loss: 0.0060 - val_loss: 61.9136 - val_kl_loss: 9.9856e-04\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 61.85958\n",
            "Epoch 94/100\n",
            " - 119s - loss: 61.3175 - kl_loss: 1.9377e-04 - val_loss: 61.8688 - val_kl_loss: 2.2256e-05\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 61.85958\n",
            "Epoch 95/100\n",
            " - 119s - loss: 61.4342 - kl_loss: 0.0050 - val_loss: 62.0754 - val_kl_loss: 4.5717e-04\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 61.85958\n",
            "Epoch 96/100\n",
            " - 119s - loss: 61.4894 - kl_loss: 0.0137 - val_loss: 61.9698 - val_kl_loss: 3.7656e-04\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 61.85958\n",
            "Epoch 97/100\n",
            " - 119s - loss: 61.4434 - kl_loss: 0.0076 - val_loss: 61.9325 - val_kl_loss: 1.2445e-04\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 61.85958\n",
            "Epoch 98/100\n",
            " - 119s - loss: 61.3639 - kl_loss: 4.0612e-04 - val_loss: 61.8754 - val_kl_loss: 2.0314e-04\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 61.85958\n",
            "Epoch 99/100\n",
            " - 119s - loss: 61.3520 - kl_loss: 0.0023 - val_loss: 61.8836 - val_kl_loss: 5.9505e-04\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 61.85958\n",
            "Epoch 100/100\n",
            " - 119s - loss: 61.3305 - kl_loss: 2.5434e-04 - val_loss: 61.8721 - val_kl_loss: 3.4401e-05\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 61.85958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8NQ7M-VF9Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(x, z_mean)\n",
        "encoder.save('drive/My Drive/models_250/encoder32dim512hid30kvocab_loss29_val34.h5')\n",
        "\n",
        "# build a generator that can sample from the learned distribution\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
        "_x_decoded_mean = decoder_mean(_h_decoded)\n",
        "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
        "generator = Model(decoder_input, _x_decoded_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49noUguLjbRA",
        "colab_type": "code",
        "outputId": "d62352a4-e1bb-4344-a7e3-f77376dcf7f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "index2word = {v: k for k, v in word_index.items()}\n",
        "index2word[0] = 'pad'\n",
        "\n",
        "#test on a validation sentence\n",
        "sent_idx = 100\n",
        "sent_encoded = encoder.predict(data_val[sent_idx:sent_idx+2,:])\n",
        "x_test_reconstructed = generator.predict(sent_encoded, batch_size = 1)\n",
        "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])\n",
        "#np.apply_along_axis(np.max, 1, x_test_reconstructed[0])\n",
        "#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[0]))\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "print(' '.join(word_list))\n",
        "original_sent = list(np.vectorize(index2word.get)(data_val[sent_idx]))\n",
        "print(' '.join(original_sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad unk unk unk unk unk unk\n",
            "pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad can i go for the same career as my friend\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbEUBCfyj4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to parse a sentence\n",
        "def sent_parse(sentence, mat_shape):\n",
        "    sequence = tokenizer.texts_to_sequences(sentence)\n",
        "    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    return padded_sent#[padded_sent, sent_one_hot]\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect):\n",
        "    all_cosine = []\n",
        "    for sent in sent_encoded:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = sent_encoded[maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two points, integer n\n",
        "# output: n equidistant points on the line between the input points (inclusive)\n",
        "def shortest_homology(point_one, point_two, num):\n",
        "    dist_vec = point_two - point_one\n",
        "    sample = np.linspace(0, 1, num, endpoint = True)\n",
        "    hom_sample = []\n",
        "    for s in sample:\n",
        "        hom_sample.append(point_one + s * dist_vec)\n",
        "    return hom_sample\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: sentence text\n",
        "def print_latent_sentence(sent_vect):\n",
        "    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n",
        "    sent_reconstructed = generator.predict(sent_vect)\n",
        "    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS+1])\n",
        "    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    w_list = [w for w in word_list if w not in ['pad']]\n",
        "    print(' '.join(w_list))\n",
        "    #print(word_list)\n",
        "     \n",
        "def new_sents_interp(sent1, sent2, n):\n",
        "    tok_sent1 = sent_parse(sent1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    tok_sent2 = sent_parse(sent2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n",
        "    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n",
        "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
        "    for point in test_hom:\n",
        "        print_latent_sentence(point)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp8-5-UBluah",
        "colab_type": "code",
        "outputId": "8c6da53b-a2be-4fc9-b352-3779359c77fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sentence1=['where can i find india']\n",
        "mysent = sent_parse(sentence1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded = encoder.predict(mysent, batch_size = 16)\n",
        "print_latent_sentence(mysent_encoded)\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded))\n",
        "\n",
        "sentence2=['gogogo where can i find an extremely good restaurant endend']\n",
        "mysent2 = sent_parse(sentence2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\n",
        "print_latent_sentence(mysent_encoded2)\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded2))\n",
        "print('-----------------')\n",
        "\n",
        "# new_sents_interp(sentence1, sentence2, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unk unk unk unk unk unk\n",
            "unk unk unk unk unk unk\n",
            "unk unk unk unk unk unk\n",
            "unk unk unk unk unk unk\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICVovGmQRFEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}